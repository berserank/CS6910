{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "36603b73",
      "metadata": {
        "id": "36603b73"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-Processing**"
      ],
      "metadata": {
        "id": "4r2LM0FSQZJq"
      },
      "id": "4r2LM0FSQZJq"
    },
    {
      "cell_type": "code",
      "source": [
        "(X, y), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X = X.reshape(X.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "print(y)\n",
        "#One Hot Encoding for y\n",
        "def one_hot_encode(y):\n",
        "  encoded_array = np.zeros((y.size, y.max()+1), dtype=int)\n",
        "  encoded_array[np.arange(y.size),y] = 1 \n",
        "  return encoded_array\n",
        "\n",
        "X = X/255\n",
        "X_test = X_test/255\n",
        "y = one_hot_encode(y)\n",
        "y_test = one_hot_encode(y_test)\n",
        "\n",
        "\n",
        "X = X.T\n",
        "X_test = X_test.T\n",
        "y = y.T"
      ],
      "metadata": {
        "id": "yCRZSoXLOgDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2476c106-2b5b-4229-9003-65853b8cf8a6"
      },
      "id": "yCRZSoXLOgDK",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9 0 0 ... 3 0 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b5e764",
      "metadata": {
        "id": "f1b5e764"
      },
      "source": [
        "**Activation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "181c79d9",
      "metadata": {
        "id": "181c79d9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    #print(-z)\n",
        "    return 1 / (1 + np.exp(-(z)))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z):\n",
        "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
        "\n",
        "def softmax(x):\n",
        "    # x = np.float128(x)\n",
        "    temp = np.exp(x-np.max(x, axis = 0))\n",
        "    fin = temp/temp.sum(axis = 0)\n",
        "    \n",
        "    return fin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92521085",
      "metadata": {
        "id": "92521085"
      },
      "source": [
        "**Weight Initialisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7b6e46f4",
      "metadata": {
        "id": "7b6e46f4"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(input_size, n, output_size):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.random.randn(n[0],input_size)*0.01\n",
        "    parameters['b'+str(1)] = np.random.randn(n[0],1)\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.random.randn(n[i],n[i-1])*0.01\n",
        "        parameters['b'+str(i+1)] = np.random.randn(n[i],1)\n",
        "    parameters['W'+str(len(n)+1)] = np.random.randn(output_size,n[-1])*0.01\n",
        "    parameters['b'+str(len(n)+1)] = np.random.randn(output_size,1)\n",
        "    return parameters\n",
        "\n",
        "def initialize_parameters_zeros(input_size, n, output_size):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.zeros((n[0],input_size))\n",
        "    parameters['b'+str(1)] = np.zeros((n[0],1))\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.zeros((n[i],n[i-1]))\n",
        "        parameters['b'+str(i+1)] = np.zeros((n[i],1))\n",
        "    parameters['W'+str(len(n)+1)] = np.zeros((output_size,n[-1]))\n",
        "    parameters['b'+str(len(n)+1)] = np.zeros((output_size,1))\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "affa846c",
      "metadata": {
        "id": "affa846c"
      },
      "source": [
        "**Forward Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c2eb9262",
      "metadata": {
        "id": "c2eb9262"
      },
      "outputs": [],
      "source": [
        "def linear(W, X, b, activation_func):\n",
        "    #print(f\"W Shape = {W.shape}, X Shape= {X.shape}, W= {W}, X = {X}, b = {b} \" )\n",
        "    h = np.matmul(W,X)+b\n",
        "    if activation_func == 'sigmoid':\n",
        "        #print(h)\n",
        "        a = sigmoid(h)\n",
        "    elif activation_func == 'relu':\n",
        "        a = relu(h)\n",
        "    elif activation_func == 'tanh':\n",
        "        a = tanh(h)\n",
        "    elif activation_func == 'softmax':\n",
        "        a = softmax(h)\n",
        "    return h,a\n",
        "\n",
        "def ForwardPropagation(X, parameters, activation_func):\n",
        "    layer_wise_outputs = {}\n",
        "    layer_wise_outputs['h1'], layer_wise_outputs['a1'] = linear(parameters['W1'], X, parameters['b1'], activation_func[0])\n",
        "    for i in range(1, (len(parameters)//2)):\n",
        "        layer_wise_outputs['h'+str(i+1)], layer_wise_outputs['a'+str(i+1)] = linear(parameters['W'+str(i+1)],layer_wise_outputs['a'+str(i)],parameters['b'+str(i+1)], activation_func[i])\n",
        "    return layer_wise_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f21563",
      "metadata": {
        "id": "00f21563"
      },
      "source": [
        "**Loss Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1f926743",
      "metadata": {
        "id": "1f926743"
      },
      "outputs": [],
      "source": [
        "def MSELoss(Y, Y_pred):\n",
        "    MSE = np.mean((Y - Y_pred) ** 2)\n",
        "    return MSE\n",
        "\n",
        "def CrossEntropyLoss(Y, Y_pred):\n",
        "    CE = [-Y[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "    crossEntropy = np.mean(CE)\n",
        "    return crossEntropy\n",
        "\n",
        "def cost(Y, Y_pred, loss_func):\n",
        "    if (loss_func == 'MSE'):\n",
        "        return (MSELoss(Y, Y_pred))\n",
        "    elif (loss_func == 'CE'):\n",
        "        return (CrossEntropyLoss(Y, Y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4612eae7",
      "metadata": {
        "id": "4612eae7"
      },
      "source": [
        "**Back Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "6b0f3183",
      "metadata": {
        "id": "6b0f3183"
      },
      "outputs": [],
      "source": [
        "def ActivationBackward(dA, Z, activation_func) :\n",
        "    \n",
        "    if (activation_func == 'sigmoid'):\n",
        "        grad = sigmoid(Z)*(1-sigmoid(Z))\n",
        "       \n",
        "    elif (activation_func == 'relu'):\n",
        "        grad = np.where(Z>0, 1, 0)\n",
        "        \n",
        "    elif (activation_func == 'tanh'):\n",
        "        grad = 1 - tanh(Z)**2\n",
        "    elif (activation_func == 'softmax'):\n",
        "        grad = softmax(Z) * (1-softmax(Z))\n",
        "    dZ = dA * grad\n",
        "    return dZ\n",
        "\n",
        "def softmax_derivative(x):\n",
        "    return softmax(x) * (1-softmax(x))        \n",
        "    \n",
        "def LayerBackward(dZl, Wl, bl, A_prev):\n",
        "    \n",
        "    m = A_prev.shape[1]\n",
        "    # print(m)\n",
        "    dWl = (1/m) * np.matmul(dZl, A_prev.T)\n",
        "    dbl = (1/m)* np.sum(dZl, axis=1, keepdims=True)\n",
        "    dA_prev = np.matmul(Wl.T,dZl)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dWl.shape == Wl.shape)\n",
        "    assert (dbl.shape == bl.shape)\n",
        "    return dWl, dbl, dA_prev\n",
        "   \n",
        "def BackPropagate(parameters, layer_wise_outputs,X, Y, activation_func, loss):\n",
        "    gradients = {}\n",
        "    l = len(layer_wise_outputs)//2\n",
        "    m = Y.shape[1]\n",
        "    AL = layer_wise_outputs['a'+str(l)]\n",
        "    HL = layer_wise_outputs['h'+str(l)]\n",
        "    \n",
        "    if loss == 'CE':\n",
        "        gradients['dh'+str(l)] = AL-Y\n",
        "    elif loss == 'MSE':\n",
        "        gradients['dh'+str(l)] = (AL-Y) * softmax_derivative(HL)\n",
        "        \n",
        "    for i in range(l-1,0,-1):\n",
        "        gradients['dW'+str(i+1)],gradients['db'+str(i+1)],gradients['da'+str(i)] = LayerBackward(gradients['dh'+str(i+1)], parameters['W'+str(i+1)], parameters['b'+str(i+1)], layer_wise_outputs['a'+str(i)])\n",
        "        gradients['dh'+str(i)] = ActivationBackward(gradients['da'+str(i)], layer_wise_outputs['h'+ str(i)] , activation_func[i-1])\n",
        "        \n",
        "    gradients['dW'+str(1)],gradients['db'+str(1)],gradients['da'+str(0)] = LayerBackward(gradients['dh'+str(1)], parameters['W'+str(1)], parameters['b'+str(1)], X)    \n",
        "    \n",
        "    return gradients\n",
        "\n",
        "\n",
        "# parameters = initialize_parameters(2, [1,2,3], 2)\n",
        "# activation_func = ['sigmoid','sigmoid','sigmoid','softmax']\n",
        "# X = np.array([1,2]).reshape(2,1)  \n",
        "# Y = np.array([1,2]).reshape(2,1)\n",
        "# loss = 'CE'\n",
        "\n",
        "# layer_wise_outputs = ForwardPropagation(X, parameters, activation_func)\n",
        "# print(layer_wise_outputs)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss))\n",
        "# print(parameters['W3'].shape)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss)['dW3'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3648a8",
      "metadata": {
        "id": "1c3648a8"
      },
      "source": [
        "**Optimisers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "90e72095",
      "metadata": {
        "id": "90e72095"
      },
      "outputs": [],
      "source": [
        "class Optimiser:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SGD(Optimiser):\n",
        "    def __init__(self, lr):\n",
        "        super().__init__(lr)\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters) // 2 \n",
        "        for l in range(1, L + 1):\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * gradients[\"db\" + str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Momentum(Optimiser):\n",
        "    def __init__(self, lr, beta):\n",
        "        super().__init__(lr)\n",
        "        self.beta = beta\n",
        "        self.v = {}\n",
        "  \n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        for l in range(1, L + 1):\n",
        "          self.v[\"W\"+str(l)] = self.beta * self.v[\"W\"+str(l)] + (1) * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * self.v[\"W\"+str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.beta * self.v[\"b\"+str(l)] + (1) * gradients[\"db\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * self.v[\"b\"+str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Nesterov(Optimiser):\n",
        "    def __init__(self, lr, gamma):\n",
        "        super().__init__(lr)\n",
        "        self.gamma = gamma\n",
        "        self.look_ahead = {}\n",
        "        self.v = {}\n",
        "        \n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        if self.look_ahead == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.look_ahead[\"W\"+str(l)] = 0\n",
        "            self.look_ahead[\"b\"+str(l)] = 0\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          self.look_ahead[\"W\"+str(l)] = parameters[\"W\" + str(l)]-self.gamma*self.v[\"W\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = self.look_ahead[\"W\"+str(l)] - self.lr * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\"+str(l)] = self.gamma * self.v[\"W\"+str(l)] + self.lr * gradients[\"dW\" + str(l)]\n",
        "\n",
        "          self.look_ahead[\"b\"+str(l)] = parameters[\"b\" + str(l)]-self.gamma*self.v[\"b\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = self.look_ahead[\"b\"+str(l)] - self.lr * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.gamma * self.v[\"b\"+str(l)] + self.lr * gradients[\"db\" + str(l)]\n",
        "\n",
        "          \n",
        "        return parameters\n",
        "\n",
        "class RMSprop(Optimiser):\n",
        "    def __init__(self, lr, decay_rate, eps):\n",
        "        super().__init__(lr)\n",
        "        self.decay_rate = decay_rate\n",
        "        self.eps = eps\n",
        "        self.s = {}\n",
        "\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.s == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.s[\"W\"+str(l)] = 0\n",
        "            self.s[\"b\"+str(l)] = 0\n",
        "        for l in range(1, L + 1):\n",
        "          self.s[\"W\"+str(l)] = self.decay_rate * self.s[\"W\"+str(l)] + (1 - self.decay_rate) * (gradients[\"dW\" + str(l)]**2)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * gradients[\"dW\" + str(l)] / (np.sqrt(self.s[\"W\"+str(l)] + self.eps))\n",
        "\n",
        "          self.s[\"b\"+str(l)] = self.decay_rate * self.s[\"b\"+str(l)] + (1 - self.decay_rate) * (gradients[\"db\" + str(l)]**2)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * gradients[\"db\" + str(l)] / (np.sqrt(self.s[\"b\"+str(l)] + self.eps))\n",
        "        \n",
        "        return parameters\n",
        "        \n",
        "\n",
        "class Adam(Optimiser):\n",
        "    def __init__(self, lr, beta1, beta2, eps):\n",
        "        super().__init__(lr)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.m == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.m[\"W\"+str(l)] = 0\n",
        "            self.m[\"b\"+str(l)] = 0\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        self.t += 1\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          self.m[\"W\" + str(l)] = self.beta1 * self.m[\"W\" + str(l)] + (1 - self.beta1) * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\" + str(l)] = self.beta2 * self.v[\"W\" + str(l)] + (1 - self.beta2) * (gradients[\"dW\" + str(l)]**2)\n",
        "          m_hat = self.m[\"W\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"W\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "          self.m[\"b\" + str(l)] = self.beta1 * self.m[\"b\" + str(l)] + (1 - self.beta1) * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\" + str(l)] = self.beta2 * self.v[\"b\" + str(l)] + (1 - self.beta2) * (gradients[\"db\" + str(l)]**2)\n",
        "          m_hat = self.m[\"b\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"b\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "        \n",
        "        \n",
        "        return parameters\n",
        "\n",
        " \n",
        "class Nadam(Optimiser):\n",
        "    def __init__(self, lr, beta1, beta2, eps):\n",
        "        super().__init__(lr)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.m == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.m[\"W\"+str(l)] = 0\n",
        "            self.m[\"b\"+str(l)] = 0\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        self.t += 1\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          self.m[\"W\" + str(l)] = self.beta1 * self.m[\"W\" + str(l)] + (1 - self.beta1) * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\" + str(l)] = self.beta2 * self.v[\"W\" + str(l)] + (1 - self.beta2) * (gradients[\"dW\" + str(l)]**2)\n",
        "          m_hat = self.m[\"W\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"W\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          m_hat_fin = (self.beta1*m_hat)+(1-self.beta1)*gradients[\"dW\" + str(l)]/(1-(self.beta1)**self.t)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * (m_hat_fin) / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "          self.m[\"b\" + str(l)] = self.beta1 * self.m[\"b\" + str(l)] + (1 - self.beta1) * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\" + str(l)] = self.beta2 * self.v[\"b\" + str(l)] + (1 - self.beta2) * (gradients[\"db\" + str(l)]**2)\n",
        "          m_hat = self.m[\"b\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"b\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          m_hat_fin = (self.beta1*m_hat)+(1-self.beta1)*gradients[\"db\" + str(l)]/(1-(self.beta1)**self.t)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * (m_hat_fin) / (np.sqrt(v_hat) + self.eps)\n",
        "        \n",
        "        \n",
        "        return parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = [784,[64,32],10]\n",
        "parameters_test = initialize_parameters(n[0],n[1],n[2])\n",
        "# print(parameters_test)\n",
        "parameters_test2 = parameters_test.copy()\n",
        "# print(parameters_test2)\n",
        "parameters_test3 = parameters_test2.copy()"
      ],
      "metadata": {
        "id": "OiaJgp2YCvkM"
      },
      "id": "OiaJgp2YCvkM",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Our Model on Fashion-MNIST**\n"
      ],
      "metadata": {
        "id": "xYSsaAlPX2xq"
      },
      "id": "xYSsaAlPX2xq"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "b8bfffb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8bfffb4",
        "outputId": "2b6ac0bf-cae9-4cf2-a130-160747ba9f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 1th epoch =0.2690700770405917\n",
            "Loss after 2th epoch =0.24549167079795764\n",
            "Loss after 3th epoch =0.2361356637148641\n",
            "Loss after 4th epoch =0.2323579718368236\n",
            "Loss after 5th epoch =0.23094991370268392\n",
            "Loss after 6th epoch =0.23047094142144892\n",
            "Loss after 7th epoch =0.23031994208042625\n",
            "Loss after 8th epoch =0.23027446880710173\n",
            "Loss after 9th epoch =0.23026081949712232\n",
            "Loss after 10th epoch =0.23025641902033964\n",
            "Loss after 11th epoch =0.23025466774797348\n",
            "Loss after 12th epoch =0.23025367820117348\n",
            "Loss after 13th epoch =0.23025291193179892\n",
            "Loss after 14th epoch =0.2302522128096104\n",
            "Loss after 15th epoch =0.23025153372308488\n",
            "Loss after 16th epoch =0.23025085939692908\n",
            "Loss after 17th epoch =0.23025018429623312\n",
            "Loss after 18th epoch =0.23024950609684605\n",
            "Loss after 19th epoch =0.23024882364454133\n",
            "Loss after 20th epoch =0.23024813626094698\n",
            "Loss after 21th epoch =0.2302474434808759\n",
            "Loss after 22th epoch =0.2302467449413246\n",
            "Loss after 23th epoch =0.23024604033000032\n",
            "Loss after 24th epoch =0.23024532935982775\n",
            "Loss after 25th epoch =0.23024461175578328\n",
            "Loss after 26th epoch =0.23024388724790917\n",
            "Loss after 27th epoch =0.2302431555675379\n",
            "Loss after 28th epoch =0.23024241644521296\n",
            "Loss after 29th epoch =0.23024166960950737\n",
            "Loss after 30th epoch =0.23024091478633554\n"
          ]
        }
      ],
      "source": [
        "#Model Architechture\n",
        "\n",
        "n = [784,[64,32],10]\n",
        "activation_func = ['sigmoid','sigmoid','softmax']\n",
        "loss = 'CE'\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "optimiser = SGD(lr = learning_rate)\n",
        "# optimiser = Momentum(lr = learning_rate, beta = 0.2)\n",
        "# optimiser = Nesterov(lr = learning_rate, gamma = 0.9)\n",
        "# optimiser = RMSprop(lr = learning_rate, decay_rate = 0.1,eps = 1e-6)\n",
        "# optimiser = Adam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "# optimiser = Nadam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "m = X.shape[1]\n",
        "# parameters = initialize_parameters(n[0],n[1],n[2])\n",
        "parameters = parameters_test2\n",
        "count = 0\n",
        "\n",
        "#loss array \n",
        "# losses = np.array([])\n",
        "\n",
        "while count < epochs :\n",
        "  training_loss = 0\n",
        "  count = count+1\n",
        "  previous_updates = {}\n",
        "  for i in np.arange(0, X.shape[1], batch_size):\n",
        "    batch_count = batch_size\n",
        "    if i + batch_size > X.shape[1]:\n",
        "      batch_count = X.shape[1] - i + 1\n",
        "    batch_size = batch_count\n",
        "    layer_wise_outputs = ForwardPropagation(X[:,i:i+batch_size], parameters, activation_func)  \n",
        "    gradients = BackPropagate(parameters, layer_wise_outputs, X[:,i:i+batch_size], y[:,i:i+batch_size], activation_func, loss)\n",
        "    parameters = optimiser.update(parameters, gradients)\n",
        "    training_loss = training_loss + cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(len(n))],loss)\n",
        "  print(\"Loss after \"+ str(count) +\"th epoch =\" +str(training_loss*(batch_size)/m))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_outputs = ForwardPropagation(X_test, parameters, activation_func)\n",
        "\n",
        "def softmax_to_label(softmax_output):\n",
        "    max_index = np.argmax(softmax_output, axis = 0)\n",
        "    return max_index\n",
        "\n",
        "test_outputs['a'+str(len(n))] = softmax_to_label(test_outputs['a'+str(len(n))])\n",
        "\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    total = len(y_true)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "print(softmax_to_label(y_test.T).shape)\n",
        "print(test_outputs['a'+str(len(n))].shape)\n",
        "y_test_fin = softmax_to_label(y_test.T)\n",
        "print(f\"Test Accuracy = {100*accuracy_score( y_test_fin, test_outputs['a'+str(len(n))])} %\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W2jKXIMuF57",
        "outputId": "20d4113e-e764-472d-cc3b-10b41de2bc29"
      },
      "id": "9W2jKXIMuF57",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n",
            "(10000,)\n",
            "Test Accuracy = 88.1 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1,2],[4,3],[]])\n",
        "np.square(A)"
      ],
      "metadata": {
        "id": "-41lvw0ewvnY"
      },
      "id": "-41lvw0ewvnY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B = A - np.max(A,axis = 0)"
      ],
      "metadata": {
        "id": "CJjit6YrwyOr"
      },
      "id": "CJjit6YrwyOr",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B / B.sum(axis = 0)"
      ],
      "metadata": {
        "id": "7a2g_NPfp8eG"
      },
      "id": "7a2g_NPfp8eG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}