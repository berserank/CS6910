{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36603b73",
      "metadata": {
        "id": "36603b73"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-Processing**"
      ],
      "metadata": {
        "id": "4r2LM0FSQZJq"
      },
      "id": "4r2LM0FSQZJq"
    },
    {
      "cell_type": "code",
      "source": [
        "(X, y), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X = X.reshape(X.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "\n",
        "#One Hot Encoding for y\n",
        "def one_hot_encode(y):\n",
        "  encoded_array = np.zeros((y.size, y.max()+1), dtype=int)\n",
        "  encoded_array[np.arange(y.size),y] = 1 \n",
        "  return encoded_array\n",
        "\n",
        "X = X/255\n",
        "X_test = X_test/255\n",
        "y = one_hot_encode(y)\n",
        "y_test = one_hot_encode(y_test)\n",
        "\n",
        "\n",
        "X = X.T\n",
        "X_test = X_test.T\n",
        "y = y.T"
      ],
      "metadata": {
        "id": "yCRZSoXLOgDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97e3300-109b-4de3-f556-06e140babec0"
      },
      "id": "yCRZSoXLOgDK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b5e764",
      "metadata": {
        "id": "f1b5e764"
      },
      "source": [
        "**Activation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "181c79d9",
      "metadata": {
        "id": "181c79d9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    #print(-z)\n",
        "    return 1 / (1 + np.exp(-(z)))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z):\n",
        "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92521085",
      "metadata": {
        "id": "92521085"
      },
      "source": [
        "**Weight Initialisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6e46f4",
      "metadata": {
        "id": "7b6e46f4"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(input_size, n, output_size):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.random.randn(n[0],input_size)*0.001\n",
        "    parameters['b'+str(1)] = np.random.randn(n[0],1)\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.random.randn(n[i],n[i-1])*0.001\n",
        "        parameters['b'+str(i+1)] = np.random.randn(n[i],1)\n",
        "    parameters['W'+str(len(n)+1)] = np.random.randn(output_size,n[-1])*0.001\n",
        "    parameters['b'+str(len(n)+1)] = np.random.randn(output_size,1)\n",
        "    return parameters\n",
        "\n",
        "def initialize_parameters_zeros(input_size, n, output_size):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.zeros((n[0],input_size))\n",
        "    parameters['b'+str(1)] = np.zeros((n[0],1))\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.zeros((n[i],n[i-1]))\n",
        "        parameters['b'+str(i+1)] = np.zeros((n[i],1))\n",
        "    parameters['W'+str(len(n)+1)] = np.zeros((output_size,n[-1]))\n",
        "    parameters['b'+str(len(n)+1)] = np.zeros((output_size,1))\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "affa846c",
      "metadata": {
        "id": "affa846c"
      },
      "source": [
        "**Forward Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2eb9262",
      "metadata": {
        "id": "c2eb9262"
      },
      "outputs": [],
      "source": [
        "def linear(W, X, b, activation_func):\n",
        "    #print(f\"W Shape = {W.shape}, X Shape= {X.shape}, W= {W}, X = {X}, b = {b} \" )\n",
        "    h = np.matmul(W,X)+b\n",
        "    if activation_func == 'sigmoid':\n",
        "        #print(h)\n",
        "        a = sigmoid(h)\n",
        "    elif activation_func == 'relu':\n",
        "        a = relu(h)\n",
        "    elif activation_func == 'tanh':\n",
        "        a = tanh(h)\n",
        "    elif activation_func == 'softmax':\n",
        "        a = softmax(h)\n",
        "    return h,a\n",
        "\n",
        "def ForwardPropagation(X, parameters, activation_func):\n",
        "    layer_wise_outputs = {}\n",
        "    layer_wise_outputs['h1'], layer_wise_outputs['a1'] = linear(parameters['W1'], X, parameters['b1'], activation_func[0])\n",
        "    for i in range(1, (len(parameters)//2)):\n",
        "        layer_wise_outputs['h'+str(i+1)], layer_wise_outputs['a'+str(i+1)] = linear(parameters['W'+str(i+1)],layer_wise_outputs['a'+str(i)],parameters['b'+str(i+1)], activation_func[i])\n",
        "    return layer_wise_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f21563",
      "metadata": {
        "id": "00f21563"
      },
      "source": [
        "**Loss Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f926743",
      "metadata": {
        "id": "1f926743"
      },
      "outputs": [],
      "source": [
        "def MSELoss(Y, Y_pred):\n",
        "    MSE = np.mean((Y - Y_pred) ** 2)\n",
        "    return MSE\n",
        "\n",
        "def CrossEntropyLoss(Y, Y_pred):\n",
        "    CE = [-Y[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "    crossEntropy = np.mean(CE)\n",
        "    return crossEntropy\n",
        "\n",
        "def cost(Y, Y_pred, loss_func):\n",
        "    if (loss_func == 'MSE'):\n",
        "        return (MSELoss(Y, Y_pred))\n",
        "    elif (loss_func == 'CE'):\n",
        "        return (CrossEntropyLoss(Y, Y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4612eae7",
      "metadata": {
        "id": "4612eae7"
      },
      "source": [
        "**Back Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0f3183",
      "metadata": {
        "id": "6b0f3183"
      },
      "outputs": [],
      "source": [
        "def ActivationBackward(dA, Z, activation_func) :\n",
        "    \n",
        "    if (activation_func == 'sigmoid'):\n",
        "        grad = sigmoid(Z)*(1-sigmoid(Z))\n",
        "       \n",
        "    elif (activation_func == 'relu'):\n",
        "        grad = np.where(Z>0, 1, 0)\n",
        "        \n",
        "    elif (activation_func == 'tanh'):\n",
        "        grad = 1 - tanh(Z)**2\n",
        "    elif (activation_func == 'softmax'):\n",
        "        grad = softmax(Z) * (1-softmax(Z))\n",
        "    dZ = dA * grad\n",
        "    return dZ\n",
        "\n",
        "def softmax_derivative(x):\n",
        "    return softmax(x) * (1-softmax(x))        \n",
        "    \n",
        "def LayerBackward(dZl, Wl, bl, A_prev):\n",
        "    \n",
        "    m = A_prev.shape[1]\n",
        "    # print(m)\n",
        "    dWl = (1/m) * np.matmul(dZl, A_prev.T)\n",
        "    dbl = (1/m)* np.sum(dZl, axis=1, keepdims=True)\n",
        "    dA_prev = np.matmul(Wl.T,dZl)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dWl.shape == Wl.shape)\n",
        "    assert (dbl.shape == bl.shape)\n",
        "    return dWl, dbl, dA_prev\n",
        "   \n",
        "def BackPropagate(parameters, layer_wise_outputs,X, Y, activation_func, loss):\n",
        "    gradients = {}\n",
        "    l = len(layer_wise_outputs)//2\n",
        "    m = Y.shape[1]\n",
        "    AL = layer_wise_outputs['a'+str(l)]\n",
        "    HL = layer_wise_outputs['h'+str(l)]\n",
        "    \n",
        "    if loss == 'CE':\n",
        "        gradients['dh'+str(l)] = AL-Y\n",
        "    elif loss == 'MSE':\n",
        "        gradients['dh'+str(l)] = (AL-Y) * softmax_derivative(HL)\n",
        "        \n",
        "    for i in range(l-1,0,-1):\n",
        "        gradients['dW'+str(i+1)],gradients['db'+str(i+1)],gradients['da'+str(i)] = LayerBackward(gradients['dh'+str(i+1)], parameters['W'+str(i+1)], parameters['b'+str(i+1)], layer_wise_outputs['a'+str(i)])\n",
        "        gradients['dh'+str(i)] = ActivationBackward(gradients['da'+str(i)], layer_wise_outputs['h'+ str(i)] , activation_func[i-1])\n",
        "        \n",
        "    gradients['dW'+str(1)],gradients['db'+str(1)],gradients['da'+str(0)] = LayerBackward(gradients['dh'+str(1)], parameters['W'+str(1)], parameters['b'+str(1)], X)    \n",
        "    \n",
        "    return gradients\n",
        "\n",
        "\n",
        "# parameters = initialize_parameters(2, [1,2,3], 2)\n",
        "# activation_func = ['sigmoid','sigmoid','sigmoid','softmax']\n",
        "# X = np.array([1,2]).reshape(2,1)  \n",
        "# Y = np.array([1,2]).reshape(2,1)\n",
        "# loss = 'CE'\n",
        "\n",
        "# layer_wise_outputs = ForwardPropagation(X, parameters, activation_func)\n",
        "# print(layer_wise_outputs)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss))\n",
        "# print(parameters['W3'].shape)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss)['dW3'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3648a8",
      "metadata": {
        "id": "1c3648a8"
      },
      "source": [
        "**Optimisers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e72095",
      "metadata": {
        "id": "90e72095"
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, lr):\n",
        "        super().__init__(lr)\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters) // 2 \n",
        "        for l in range(1, L + 1):\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * gradients[\"db\" + str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Momentum(Optimizer):\n",
        "    def __init__(self, lr, beta, L):\n",
        "        super().__init__(lr)\n",
        "        self.beta = beta\n",
        "        self.L = L\n",
        "        self.v = {}\n",
        "        for l in range(1, self.L + 1):\n",
        "          self.v[\"W\"+str(l)] = 0\n",
        "          self.v[\"b\"+str(l)] = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        for l in range(1, self.L + 1):\n",
        "          self.v[\"W\"+str(l)] = self.beta * self.v[\"W\"+str(l)] + (1) * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * self.v[\"W\"+str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.beta * self.v[\"b\"+str(l)] + (1) * gradients[\"db\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * self.v[\"b\"+str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Nesterov(Optimizer):\n",
        "    def __init__(self, lr, beta, L):\n",
        "        super().__init__(lr)\n",
        "        self.beta = beta\n",
        "        self.L = L\n",
        "        self.v = {}\n",
        "        self.v_prev = {}\n",
        "        for l in range(1, self.L + 1):\n",
        "          self.v[\"W\"+str(l)] = 0\n",
        "          self.v[\"b\"+str(l)] = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        for l in range(1, self.L + 1):\n",
        "          self.v_prev[\"W\"+str(l)] = self.v[\"W\"+str(l)]\n",
        "          self.v[\"W\"+str(l)] = self.beta * self.v[\"W\"+str(l)] - self.lr * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)]+ (-self.beta * self.v_prev[\"W\"+str(l)] + (1 + self.beta) * self.v[\"W\"+str(l)])\n",
        "\n",
        "          self.v_prev[\"b\"+str(l)] = self.v[\"b\"+str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.beta * self.v[\"b\"+str(l)] - self.lr * gradients[\"db\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)]+ (-self.beta * self.v_prev[\"b\"+str(l)] + (1 + self.beta) * self.v[\"W\"+str(l)])\n",
        "\n",
        "        return parameters\n",
        "\n",
        "class RMSprop(Optimizer):\n",
        "    def __init__(self, lr, decay_rate, eps, L):\n",
        "        super().__init__(lr)\n",
        "        self.decay_rate = decay_rate\n",
        "        self.eps = eps\n",
        "        self.L = L\n",
        "        self.s = {}\n",
        "        for l in range(1, self.L + 1):\n",
        "          self.s[\"W\"+str(l)] = 0\n",
        "          self.s[\"b\"+str(l)] = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        for l in range(1, self.L + 1):\n",
        "          self.s[\"W\"+str(l)] = self.decay_rate * self.s[\"W\"+str(l)] + (1 - self.decay_rate) * (gradients[\"dW\" + str(l)]**2)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * gradients[\"dW\" + str(l)] / (np.sqrt(self.s[\"W\"+str(l)] + self.eps))\n",
        "\n",
        "          self.s[\"b\"+str(l)] = self.decay_rate * self.s[\"b\"+str(l)] + (1 - self.decay_rate) * (gradients[\"db\" + str(l)]**2)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * gradients[\"db\" + str(l)] / (np.sqrt(self.s[\"b\"+str(l)] + self.eps))\n",
        "        \n",
        "        return parameters\n",
        "        \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = [784,[64,32],10]\n",
        "parameters_test = initialize_parameters(n[0],n[1],n[2])\n",
        "# print(parameters_test)\n",
        "parameters_test2 = parameters_test.copy()\n",
        "# print(parameters_test2)"
      ],
      "metadata": {
        "id": "OiaJgp2YCvkM"
      },
      "id": "OiaJgp2YCvkM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Our Model on Fashion-MNIST**\n"
      ],
      "metadata": {
        "id": "xYSsaAlPX2xq"
      },
      "id": "xYSsaAlPX2xq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8bfffb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "b8bfffb4",
        "outputId": "7a68cff2-49d9-4de0-efbe-89b15be64e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 1th epoch =0.10843772307857845\n",
            "Loss after 2th epoch =0.08658647022123446\n",
            "Loss after 3th epoch =0.08075015865309312\n",
            "Loss after 4th epoch =0.08149409220018064\n",
            "Loss after 5th epoch =0.08027422705365513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-9d588df25a47>:3: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-(z)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 6th epoch =0.07893272192805346\n",
            "Loss after 7th epoch =0.07776554442911045\n",
            "Loss after 8th epoch =0.07714840471446399\n",
            "Loss after 9th epoch =0.08035753865838034\n",
            "Loss after 10th epoch =0.07916909174587049\n",
            "Loss after 11th epoch =0.07976656278025464\n",
            "Loss after 12th epoch =0.07919409906765809\n",
            "Loss after 13th epoch =0.08022237534251431\n",
            "Loss after 14th epoch =0.07886531178738959\n",
            "Loss after 15th epoch =0.0821309030266599\n",
            "Loss after 16th epoch =0.08083217200314766\n",
            "Loss after 17th epoch =0.07859436272775482\n",
            "Loss after 18th epoch =0.07850973281188695\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-e60c1910b070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mlayer_wise_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mForwardPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBackPropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_wise_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_wise_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-242b1589b679>\u001b[0m in \u001b[0;36mBackPropagate\u001b[0;34m(parameters, layer_wise_outputs, X, Y, activation_func, loss)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dh'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivationBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'da'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_wise_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mactivation_func\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dW'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'da'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dh'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-242b1589b679>\u001b[0m in \u001b[0;36mLayerBackward\u001b[0;34m(dZl, Wl, bl, A_prev)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdWl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdZl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Model Architechture\n",
        "\n",
        "n = [784,[64,32],10]\n",
        "activation_func = ['sigmoid','sigmoid','softmax']\n",
        "loss = 'CE'\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "# optimizer = SGD(lr = learning_rate)\n",
        "# optimizer = Momentum(lr = learning_rate, beta = 0.2, L = len(n[1])+1)\n",
        "# optimizer = Nesterov(lr = learning_rate, beta = 0.9, L = len(n[1])+1)\n",
        "optimizer = RMSprop(lr = learning_rate, decay_rate = 0.1,eps = 1e-8, L = len(n[1])+1)\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "m = X.shape[1]\n",
        "parameters = initialize_parameters(n[0],n[1],n[2])\n",
        "# parameters = parameters_test2\n",
        "count = 0\n",
        "\n",
        "#loss array \n",
        "# losses = np.array([])\n",
        "\n",
        "while count < epochs :\n",
        "  training_loss = 0\n",
        "  count = count+1\n",
        "  previous_updates = {}\n",
        "  for i in np.arange(0, X.shape[1], batch_size):\n",
        "    batch_count = batch_size\n",
        "    if i + batch_size > X.shape[1]:\n",
        "      batch_count = X.shape[1] - i + 1\n",
        "    batch_size = batch_count\n",
        "    layer_wise_outputs = ForwardPropagation(X[:,i:i+batch_size], parameters, activation_func)  \n",
        "    gradients = BackPropagate(parameters, layer_wise_outputs, X[:,i:i+batch_size], y[:,i:i+batch_size], activation_func, loss)\n",
        "    parameters = optimizer.update(parameters, gradients)\n",
        "    training_loss = training_loss + cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(len(n))],loss)\n",
        "  print(\"Loss after \"+ str(count) +\"th epoch =\" +str(training_loss*(batch_size)/m))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_outputs = ForwardPropagation(X_test, parameters, activation_func)\n",
        "\n",
        "def softmax_to_label(softmax_output):\n",
        "    max_index = np.argmax(softmax_output, axis = 0)\n",
        "    return max_index\n",
        "\n",
        "test_outputs['a'+str(len(n))] = softmax_to_label(test_outputs['a'+str(len(n))])\n",
        "\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    total = len(y_true)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "print(softmax_to_label(y_test.T).shape)\n",
        "print(test_outputs['a'+str(len(n))].shape)\n",
        "y_test_fin = softmax_to_label(y_test.T)\n",
        "print(f\"Test Accuracy = {100*accuracy_score( y_test_fin, test_outputs['a'+str(len(n))])} %\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W2jKXIMuF57",
        "outputId": "6f94e628-d6e7-4326-ca02-1ebc0caf6e19"
      },
      "id": "9W2jKXIMuF57",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n",
            "(10000,)\n",
            "Test Accuracy = 78.83 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-9d588df25a47>:3: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-(z)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1,2]])\n",
        "np.square(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-41lvw0ewvnY",
        "outputId": "2e27609d-86ed-4393-ee0f-4c7bd8866a2b"
      },
      "id": "-41lvw0ewvnY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJjit6YrwyOr"
      },
      "id": "CJjit6YrwyOr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}