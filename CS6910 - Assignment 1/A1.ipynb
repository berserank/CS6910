{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "T7vQy_csoJzQ"
      },
      "id": "T7vQy_csoJzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "lLhrqoDeojLz"
      },
      "id": "lLhrqoDeojLz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36603b73",
      "metadata": {
        "id": "36603b73"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist, mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "wandb.init(project=\"CS6910 Assignment 1\")"
      ],
      "metadata": {
        "id": "v_fnfKBJEIhw"
      },
      "id": "v_fnfKBJEIhw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Loading Data**"
      ],
      "metadata": {
        "id": "4r2LM0FSQZJq"
      },
      "id": "4r2LM0FSQZJq"
    },
    {
      "cell_type": "code",
      "source": [
        "(X, y), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "yCRZSoXLOgDK"
      },
      "id": "yCRZSoXLOgDK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1 - Plotting MNIST Fashion Dataset**"
      ],
      "metadata": {
        "id": "lJs0eq3Xpat7"
      },
      "id": "lJs0eq3Xpat7"
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "fig, ax = plt.subplots(2,5,figsize=(20, 10))\n",
        "ax = ax.reshape(-1)\n",
        "\n",
        "for i in range(10):\n",
        "  sample = np.array(np.where(y == i)).reshape(-1)\n",
        "  sample = sample[3]\n",
        "  image = X[sample]\n",
        "  ax[i].set_title(classes[i])\n",
        "  ax[i].imshow(image)\n",
        "  \n",
        "plt.show()\n",
        "wandb.log({'Labels':fig})"
      ],
      "metadata": {
        "id": "Xvk0PjtzpiZE"
      },
      "id": "Xvk0PjtzpiZE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Helper Functions - 1**"
      ],
      "metadata": {
        "id": "8FSpqS1yK-ax"
      },
      "id": "8FSpqS1yK-ax"
    },
    {
      "cell_type": "code",
      "source": [
        "#One Hot Encoding for y\n",
        "def one_hot_encode(y):\n",
        "  encoded_array = np.zeros((y.size, y.max()+1), dtype=int)\n",
        "  encoded_array[np.arange(y.size),y] = 1 \n",
        "  return encoded_array\n",
        "\n",
        "#Converts softmax probabilities to labels\n",
        "def softmax_to_label(softmax_output):\n",
        "  max_index = np.argmax(softmax_output, axis = 0)\n",
        "  return max_index\n",
        "\n"
      ],
      "metadata": {
        "id": "WqY1J3bULCQO"
      },
      "id": "WqY1J3bULCQO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Pre-Processing**\n",
        "\n",
        "In order to train this model, I have set the input vector X to have a size of (784, 60000), whereas initially it was a vector of shape (60000, 28, 28). To achieve this, I utilized the reshape() function and then normalized the input by dividing it by 255.\n",
        "\n",
        "Likewise, I performed one hot encoding on the label sequence y. However, please note that I did not encode y_test, as it simplifies the accuracy calculation process when evaluating acccuracy on test data."
      ],
      "metadata": {
        "id": "QsXHPBlepobh"
      },
      "id": "QsXHPBlepobh"
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.reshape(X.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "\n",
        "X = X/255\n",
        "X_test = X_test/255\n",
        "y = one_hot_encode(y)\n",
        "\n",
        "\n",
        "X = X.T\n",
        "X_test = X_test.T\n",
        "y = y.T"
      ],
      "metadata": {
        "id": "WaHeSKrhH0nK"
      },
      "id": "WaHeSKrhH0nK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f1b5e764",
      "metadata": {
        "id": "f1b5e764"
      },
      "source": [
        "### **Activation Functions**\n",
        "\n",
        "Here are the 4 activation functions I used for my network. \n",
        "\n",
        "While dealing with MNIST/ Fashion-MNIST , output layer will have softmax as it's activation function inorder to output a vector of probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "181c79d9",
      "metadata": {
        "id": "181c79d9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    #print(-z)\n",
        "    return 1 / (1 + np.exp(-(z)))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z):\n",
        "    return (z>0)*(z) + ((z<0)*0)\n",
        "\n",
        "def leakyRelu(z):\n",
        "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
        "\n",
        "def softmax(x):\n",
        "    # x = np.float128(x)\n",
        "    temp = np.exp(x-np.max(x, axis = 0))\n",
        "    fin = temp/temp.sum(axis = 0)\n",
        "    return fin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92521085",
      "metadata": {
        "id": "92521085"
      },
      "source": [
        "### **Weight Initialisation**\n",
        "\n",
        "Attaching the reference I used for implementing Xavier initialisation: [Xavier Initialisation](https://cs230.stanford.edu/section/4/)\n",
        "\n",
        "This initialises a dictionary of parameters of length = *hidden layer size + 1*\n",
        "\n",
        "\n",
        "> **Parameters:** { 'W1' : $W_{(n[0],X)}$ , 'b1' : $b_{(n[0],1)}$ , 'W2' : $W_{(n[1],n[0])}$ , 'b2' : $b_{(n[1],1)}$ , 'W2' : $W_{(n[2],n[1])}$, 'b2' : $b_{(n[2],1)}$ , . . .  .  .  }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "where n is the vector of hidden layer sizes.\n",
        "\n",
        "While dealing with MNIST/ Fashion-MNIST , input size is 784 and output layer size is 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6e46f4",
      "metadata": {
        "id": "7b6e46f4"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(input_size, n, output_size, initialisation):\n",
        "  if (initialisation == 'Random'):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.random.randn(n[0],input_size)*0.01\n",
        "    parameters['b'+str(1)] = np.random.randn(n[0],1)\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.random.randn(n[i],n[i-1])*0.01\n",
        "        parameters['b'+str(i+1)] = np.random.randn(n[i],1)\n",
        "    parameters['W'+str(len(n)+1)] = np.random.randn(output_size,n[-1])*0.01\n",
        "    parameters['b'+str(len(n)+1)] = np.random.randn(output_size,1)\n",
        "\n",
        "  elif (initialisation == 'Xavier'):\n",
        "    parameters = {}\n",
        "    m = np.sqrt(6)/(input_size+n[0])\n",
        "    parameters['W'+str(1)] = np.random.uniform(-m,m, (n[0],input_size))\n",
        "    parameters['b'+str(1)] = np.random.randn(n[0],1)\n",
        "    for i in range(1,len(n)):\n",
        "        m = np.sqrt(6)/(n[i-1]+n[i])\n",
        "        parameters['W'+str(i+1)] = np.random.uniform(-m,m, (n[i],n[i-1]) )\n",
        "        parameters['b'+str(i+1)] = np.random.randn(n[i],1)\n",
        "    m = np.sqrt(6)/(output_size+n[-1])\n",
        "    parameters['W'+str(len(n)+1)] = np.random.uniform(-m,m,(output_size,n[-1]))\n",
        "    parameters['b'+str(len(n)+1)] = np.random.randn(output_size,1)\n",
        "\n",
        "  return parameters\n",
        "\n",
        "def initialize_parameters_zeros(input_size, n, output_size):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.zeros((n[0],input_size))\n",
        "    parameters['b'+str(1)] = np.zeros((n[0],1))\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.zeros((n[i],n[i-1]))\n",
        "        parameters['b'+str(i+1)] = np.zeros((n[i],1))\n",
        "    parameters['W'+str(len(n)+1)] = np.zeros((output_size,n[-1]))\n",
        "    parameters['b'+str(len(n)+1)] = np.zeros((output_size,1))\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "affa846c",
      "metadata": {
        "id": "affa846c"
      },
      "source": [
        "### **Forward Propagation**\n",
        "\n",
        "This module takes the parameters dictionary, sequence of activation functions, input vector as inputs and outputs a dictionary of layer wise outputs.\n",
        "\n",
        "\n",
        "\n",
        "> **Layer Wise Outputs:** { 'h1' : $h_{n[0]}$ ,  'a1' : $a_{n[0]}$ , 'h2' : $h_{n[1]}$ ,  'a2' : $a_{n[1]}$ ,  'h3' : $h_{n[2]}$ ,  'a3' : $a_{n[2]}$ , . . .  .  .  }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Where, h is pre-activation output and a is post-activation output of a particular layer. If g is the activation function, this module basically does,\n",
        "\n",
        "\n",
        "\n",
        ">$h_i = W_ia_{i-1} + b_i$   \n",
        "\n",
        "> $g(h_i) = a_i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2eb9262",
      "metadata": {
        "id": "c2eb9262"
      },
      "outputs": [],
      "source": [
        "def linear(W, X, b, activation_func):\n",
        "    #print(f\"W Shape = {W.shape}, X Shape= {X.shape}, W= {W}, X = {X}, b = {b} \" )\n",
        "    h = np.matmul(W,X)+b\n",
        "    if activation_func == 'sigmoid':\n",
        "        #print(h)\n",
        "        a = sigmoid(h)\n",
        "    elif activation_func == 'relu':\n",
        "        a = relu(h)\n",
        "    elif activation_func == 'leakyRelu':\n",
        "        a = leakyRelu(h)\n",
        "    elif activation_func == 'tanh':\n",
        "        a = tanh(h)\n",
        "    elif activation_func == 'softmax':\n",
        "        a = softmax(h)\n",
        "    return h,a\n",
        "\n",
        "def ForwardPropagation(X, parameters, activation_func):\n",
        "    layer_wise_outputs = {}\n",
        "    layer_wise_outputs['h1'], layer_wise_outputs['a1'] = linear(parameters['W1'], X, parameters['b1'], activation_func[0])\n",
        "    for i in range(1, (len(parameters)//2)):\n",
        "        layer_wise_outputs['h'+str(i+1)], layer_wise_outputs['a'+str(i+1)] = linear(parameters['W'+str(i+1)],layer_wise_outputs['a'+str(i)],parameters['b'+str(i+1)], activation_func[i])\n",
        "    return layer_wise_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f21563",
      "metadata": {
        "id": "00f21563"
      },
      "source": [
        "### **Loss, Cost, Accuracy Functions**\n",
        "\n",
        "Below is the code for mean square error loss, cross entropy loss, cost function, accuarcy score \n",
        "\n",
        "Notice that accuracy score takes it's *true values* as labels and *a softmax vector* as predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f926743",
      "metadata": {
        "id": "1f926743"
      },
      "outputs": [],
      "source": [
        "def MSELoss(Y, Y_pred):\n",
        "    MSE = np.mean((Y - Y_pred) ** 2, axis = 1)\n",
        "    MSE = np.mean(MSE)\n",
        "    return MSE\n",
        "\n",
        "def CrossEntropyLoss(Y, Y_pred):\n",
        "    CE = [-Y[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "    crossEntropy = np.mean(CE)\n",
        "    return crossEntropy\n",
        "\n",
        "def cost(Y, Y_pred, loss_func):\n",
        "    if (loss_func == 'MSE'):\n",
        "        return (MSELoss(Y, Y_pred))\n",
        "    elif (loss_func == 'CE'):\n",
        "        return (CrossEntropyLoss(Y, Y_pred))\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    pred_labels = np.argmax(y_pred, axis=0)\n",
        "    return np.sum(pred_labels == y_true) / len(y_true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4612eae7",
      "metadata": {
        "id": "4612eae7"
      },
      "source": [
        "### **Back Propagation**\n",
        "\n",
        "I have implemented backpropagation with 2 modules in a for loop. First module backpropagates through post-actiavtion outputs(`ActivationBackward`) and second one back propagates through pre-activation outputs(`LayerBackward`). Later the `BackPropagate` function helps us creating the `gradients` dictionary which has all the corresponding gradients of the loss function with respect to each weight and bias.\n",
        "\n",
        "\n",
        "**Activation Backward:**\n",
        "\n",
        "This module takes $\\frac{dL}{dA_l}$ as `dA`, activation function `g` of that layer as inputs and returns $\\frac{dL}{dZ_l}$ as `dZ`\n",
        "\n",
        "\n",
        "\n",
        "> $\\frac{dL}{dZ_l} = \\frac{dL}{dA_l} * g'(Z_l)$\n",
        "\n",
        "\n",
        "\n",
        "**Layer Backward:**\n",
        "\n",
        "This modules takes $\\frac{dL}{dZ_l}$ as `dZ` and $W_l, b_l, a_{l-1}$ as `Wl, bl, A_prev` as inputs and outputs the $\\frac{dL}{dA_{l-1}}$ as `dA_prev` along with the gradients with respect to weights,biases of that layer.\n",
        "\n",
        "\n",
        "\n",
        "> $\\frac{dL}{dA_{l-1}} = W_l^T.\\frac{dL}{dZ_l}$\n",
        "\n",
        "> $\\frac{dL}{dW_{l}} = (\\frac{1}{m})\\frac{dL}{dZ_l}.a_{l-1}^T $\n",
        "\n",
        "> $\\frac{dL}{db_{l}} = (\\frac{1}{m})\\Sigma_{i=1}^n \\frac{dL^i}{dZ_l}  $\n",
        "\n",
        "\n",
        "\n",
        "After computing the gradient of loss with respect to last layer's Z, we can loop through the network by implementing the above modules repeatedly. The main `BackPropagation` function reuturns a dictionary of gradients in the form, \n",
        "\n",
        "\n",
        "\n",
        "> **Gradients:** { 'dW1' : $\\frac{dL}{dW_{(n[0],X)}}$ , 'db1' : $\\frac{dL}{db_{(n[0],1)}}$ , 'dW2' : $\\frac{dL}{dW_{(n[1],n[0])}}$ , 'db2' : $\\frac{dL}{db_{(n[1],1)}}$ , 'dW2' : $\\frac{dL}{dW_{(n[2],n[1])}}$, 'db2' : $\\frac{dL}{db_{(n[2],1)}}$ , . . .  .  .  }\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0f3183",
      "metadata": {
        "id": "6b0f3183"
      },
      "outputs": [],
      "source": [
        "def ActivationBackward(dA, Z, activation_func) :\n",
        "    \n",
        "    if (activation_func == 'sigmoid'):\n",
        "        grad = sigmoid(Z)*(1-sigmoid(Z))\n",
        "       \n",
        "    elif (activation_func == 'relu'):\n",
        "        grad = np.where(Z>0, 1, 0)\n",
        "\n",
        "    elif (activation_func == 'leakyRelu'):\n",
        "        grad = np.where(Z>0, 1, 0.01)\n",
        "\n",
        "    elif (activation_func == 'tanh'):\n",
        "        grad = 1 - tanh(Z)**2\n",
        "    elif (activation_func == 'softmax'):\n",
        "        grad = softmax(Z) * (1-softmax(Z))\n",
        "    dZ = dA * grad\n",
        "    return dZ\n",
        "\n",
        "def softmax_derivative(x):\n",
        "    return softmax(x) * (1-softmax(x))        \n",
        "    \n",
        "def LayerBackward(dZl, Wl, bl, A_prev):\n",
        "    \n",
        "    m = A_prev.shape[1]\n",
        "    # print(m)\n",
        "    dWl = (1/m) * np.matmul(dZl, A_prev.T)\n",
        "    dbl = (1/m)* np.sum(dZl, axis=1, keepdims=True)\n",
        "    dA_prev = np.matmul(Wl.T,dZl)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dWl.shape == Wl.shape)\n",
        "    assert (dbl.shape == bl.shape)\n",
        "    return dWl, dbl, dA_prev\n",
        "   \n",
        "def BackPropagate(parameters, layer_wise_outputs,X, Y, activation_func, loss):\n",
        "    gradients = {}\n",
        "    l = len(layer_wise_outputs)//2\n",
        "    m = Y.shape[1]\n",
        "    AL = layer_wise_outputs['a'+str(l)]\n",
        "    HL = layer_wise_outputs['h'+str(l)]\n",
        "    \n",
        "    if loss == 'CE':\n",
        "        gradients['dh'+str(l)] = AL-Y\n",
        "    elif loss == 'MSE':\n",
        "        gradients['dh'+str(l)] = (AL-Y) * softmax_derivative(HL)\n",
        "        \n",
        "    for i in range(l-1,0,-1):\n",
        "        gradients['dW'+str(i+1)],gradients['db'+str(i+1)],gradients['da'+str(i)] = LayerBackward(gradients['dh'+str(i+1)], parameters['W'+str(i+1)], parameters['b'+str(i+1)], layer_wise_outputs['a'+str(i)])\n",
        "        gradients['dh'+str(i)] = ActivationBackward(gradients['da'+str(i)], layer_wise_outputs['h'+ str(i)] , activation_func[i-1])\n",
        "        \n",
        "    gradients['dW'+str(1)],gradients['db'+str(1)],gradients['da'+str(0)] = LayerBackward(gradients['dh'+str(1)], parameters['W'+str(1)], parameters['b'+str(1)], X)    \n",
        "    \n",
        "    return gradients\n",
        "\n",
        "\n",
        "# parameters = initialize_parameters(2, [1,2,3], 2)\n",
        "# activation_func = ['sigmoid','sigmoid','sigmoid','softmax']\n",
        "# X = np.array([1,2]).reshape(2,1)  \n",
        "# Y = np.array([1,2]).reshape(2,1)\n",
        "# loss = 'CE'\n",
        "\n",
        "# layer_wise_outputs = ForwardPropagation(X, parameters, activation_func)\n",
        "# print(layer_wise_outputs)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss))\n",
        "# print(parameters['W3'].shape)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss)['dW3'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3648a8",
      "metadata": {
        "id": "1c3648a8"
      },
      "source": [
        "###**Optimisers**\n",
        "\n",
        "I created a base class for optimizers that includes two attributes: `learning rate` and `weight decay`. Any optimizer can inherit from this class. The child classes differ in their `update` methods, and the update equations for the optimizers implemented are listed below\n",
        "\n",
        "\n",
        "**References** : [CS6910 Lecture 5](https://iitm-pod.slides.com/arunprakash_ai/cs6910-lecture-5),  [Optimiser Algorithms](https://cs229.stanford.edu/proj2015/054_report.pdf)\n",
        "\n",
        "To add a new optimiser, we need to write a new iherited class and write the update method accordingly.\n",
        "\n",
        "###**Regularisation**\n",
        "I assigned `weight decay` $λ$ as a common attribute to all the optimiser classes. I changed gradients at every update step accordingly.\n",
        "\n",
        "\\begin{align} \\nabla 𝓛(w^t) = \\nabla L(w^t) + \\lambda w_t\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "For SGD,\n",
        "\\begin{align} w_{t+1} = w_t - \\eta \\nabla 𝓛(w^t)\n",
        "\\end{align}\n",
        "\n",
        "Hence,\n",
        "\n",
        "\\begin{align} w_{t+1} = w_t - \\eta \\left(\\nabla L(w^t) + \\lambda w_t\\right)\n",
        "\\end{align}\n",
        "\n",
        "where, $\\nabla 𝓛(w^t)$ is regularised loss gradient and $\\nabla L(w^t)$ is unregularised loss gradient\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e72095",
      "metadata": {
        "id": "90e72095"
      },
      "outputs": [],
      "source": [
        "class Optimiser:\n",
        "    def __init__(self, lr, weight_decay):\n",
        "        self.lr = lr\n",
        "        self.wd = weight_decay\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SGD(Optimiser):\n",
        "    def __init__(self, lr, weight_decay):\n",
        "        super().__init__(lr,weight_decay)\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters) // 2 \n",
        "        for l in range(1, L + 1):\n",
        "          parameters[\"W\" + str(l)] = (1-self.wd*self.lr)*parameters[\"W\" + str(l)] - self.lr * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = (1-self.wd*self.lr)*parameters[\"b\" + str(l)] - self.lr * gradients[\"db\" + str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Momentum(Optimiser):\n",
        "    def __init__(self, lr, beta, weight_decay):\n",
        "        super().__init__(lr,weight_decay)\n",
        "        self.beta = beta\n",
        "        self.v = {}\n",
        "  \n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        for l in range(1, L + 1):\n",
        "          gradients[\"dW\" + str(l)]= gradients[\"dW\" + str(l)]+self.wd*parameters[\"W\" + str(l)]\n",
        "          gradients[\"db\" + str(l)] = gradients[\"db\" + str(l)]+self.wd*parameters[\"b\" + str(l)]\n",
        "          \n",
        "          self.v[\"W\"+str(l)] = self.beta * self.v[\"W\"+str(l)] + (1-self.beta) * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * self.v[\"W\"+str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.beta * self.v[\"b\"+str(l)] + (1-self.beta) * (gradients[\"db\" + str(l)])\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * self.v[\"b\"+str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Nesterov(Optimiser):\n",
        "    def __init__(self, lr, gamma,weight_decay):\n",
        "        super().__init__(lr,weight_decay)\n",
        "        self.gamma = gamma\n",
        "        self.look_ahead = {}\n",
        "        self.v = {}\n",
        "        \n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        if self.look_ahead == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.look_ahead[\"W\"+str(l)] = 0\n",
        "            self.look_ahead[\"b\"+str(l)] = 0\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          gradients[\"dW\" + str(l)]= gradients[\"dW\" + str(l)]+self.wd*parameters[\"W\" + str(l)]\n",
        "          gradients[\"db\" + str(l)] = gradients[\"db\" + str(l)]+self.wd*parameters[\"b\" + str(l)]\n",
        "            \n",
        "          self.look_ahead[\"W\"+str(l)] = parameters[\"W\" + str(l)]-self.gamma*self.v[\"W\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = self.look_ahead[\"W\"+str(l)] - self.lr * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\"+str(l)] = self.gamma * self.v[\"W\"+str(l)] + self.lr * gradients[\"dW\" + str(l)]\n",
        "\n",
        "          self.look_ahead[\"b\"+str(l)] = parameters[\"b\" + str(l)]-self.gamma*self.v[\"b\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = self.look_ahead[\"b\"+str(l)] - self.lr * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.gamma * self.v[\"b\"+str(l)] + self.lr * gradients[\"db\" + str(l)]\n",
        "\n",
        "          \n",
        "        return parameters\n",
        "\n",
        "class RMSprop(Optimiser):\n",
        "    def __init__(self, lr, decay_rate, eps,weight_decay):\n",
        "        super().__init__(lr,weight_decay)\n",
        "        self.decay_rate = decay_rate\n",
        "        self.eps = eps\n",
        "        self.s = {}\n",
        "\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        \n",
        "        L = len(parameters)//2\n",
        "        if self.s == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.s[\"W\"+str(l)] = 0\n",
        "            self.s[\"b\"+str(l)] = 0\n",
        "        for l in range(1, L + 1):\n",
        "          gradients[\"dW\" + str(l)]= gradients[\"dW\" + str(l)]+self.wd*parameters[\"W\" + str(l)]\n",
        "          gradients[\"db\" + str(l)] = gradients[\"db\" + str(l)]+self.wd*parameters[\"b\" + str(l)]\n",
        "            \n",
        "          self.s[\"W\"+str(l)] = self.decay_rate * self.s[\"W\"+str(l)] + (1 - self.decay_rate) * (gradients[\"dW\" + str(l)]**2)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * (gradients[\"dW\" + str(l)]) / (np.sqrt(self.s[\"W\"+str(l)]) + self.eps)\n",
        "\n",
        "          self.s[\"b\"+str(l)] = self.decay_rate * self.s[\"b\"+str(l)] + (1 - self.decay_rate) * (gradients[\"db\" + str(l)]**2)\n",
        "          parameters[\"b\" + str(l)] = (1-self.wd)*parameters[\"b\" + str(l)] - self.lr * (gradients[\"db\" + str(l)]) / (np.sqrt(self.s[\"b\"+str(l)]) + self.eps)\n",
        "        \n",
        "        return parameters\n",
        "        \n",
        "\n",
        "class Adam(Optimiser):\n",
        "    def __init__(self, lr, beta1, beta2, eps,weight_decay):\n",
        "        super().__init__(lr,weight_decay)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.m == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.m[\"W\"+str(l)] = 0\n",
        "            self.m[\"b\"+str(l)] = 0\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        self.t += 1\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          gradients[\"dW\" + str(l)]= gradients[\"dW\" + str(l)]+self.wd*parameters[\"W\" + str(l)]\n",
        "          gradients[\"db\" + str(l)] = gradients[\"db\" + str(l)]+self.wd*parameters[\"b\" + str(l)]\n",
        "            \n",
        "          self.m[\"W\" + str(l)] = self.beta1 * self.m[\"W\" + str(l)] + (1 - self.beta1) * (gradients[\"dW\" + str(l)])\n",
        "          self.v[\"W\" + str(l)] = self.beta2 * self.v[\"W\" + str(l)] + (1 - self.beta2) * ((gradients[\"dW\" + str(l)])**2)\n",
        "          m_hat = self.m[\"W\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"W\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "          self.m[\"b\" + str(l)] = self.beta1 * self.m[\"b\" + str(l)] + (1 - self.beta1) * (gradients[\"db\" + str(l)])\n",
        "          self.v[\"b\" + str(l)] = self.beta2 * self.v[\"b\" + str(l)] + (1 - self.beta2) * ((gradients[\"db\" + str(l)])**2)\n",
        "          m_hat = self.m[\"b\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"b\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "        \n",
        "        \n",
        "        return parameters\n",
        "\n",
        " \n",
        "class Nadam(Optimiser):\n",
        "    def __init__(self, lr, beta1, beta2, eps,weight_decay):\n",
        "        super().__init__(lr,weight_decay)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.m == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.m[\"W\"+str(l)] = 0\n",
        "            self.m[\"b\"+str(l)] = 0\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        self.t += 1\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          gradients[\"dW\" + str(l)]= gradients[\"dW\" + str(l)]+self.wd*parameters[\"W\" + str(l)]\n",
        "          gradients[\"db\" + str(l)] = gradients[\"db\" + str(l)]+self.wd*parameters[\"b\" + str(l)]\n",
        "            \n",
        "          self.m[\"W\" + str(l)] = self.beta1 * self.m[\"W\" + str(l)] + (1 - self.beta1) * (gradients[\"dW\" + str(l)]-self.wd*parameters[\"W\" + str(l)])\n",
        "          self.v[\"W\" + str(l)] = self.beta2 * self.v[\"W\" + str(l)] + (1 - self.beta2) * ((gradients[\"dW\" + str(l)]-self.wd*parameters[\"W\" + str(l)])**2)\n",
        "          m_hat = self.m[\"W\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"W\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          m_hat_fin = (self.beta1*m_hat)+(1-self.beta1)*gradients[\"dW\" + str(l)]/(1-(self.beta1)**self.t)\n",
        "          parameters[\"W\" + str(l)] = (1-self.wd)*parameters[\"W\" + str(l)] - self.lr * (m_hat_fin) / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "          self.m[\"b\" + str(l)] = self.beta1 * self.m[\"b\" + str(l)] + (1 - self.beta1) * (gradients[\"db\" + str(l)]-self.wd*parameters[\"b\" + str(l)])\n",
        "          self.v[\"b\" + str(l)] = self.beta2 * self.v[\"b\" + str(l)] + (1 - self.beta2) * ((gradients[\"db\" + str(l)]-self.wd*parameters[\"b\" + str(l)])**2)\n",
        "          m_hat = self.m[\"b\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"b\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          m_hat_fin = (self.beta1*m_hat)+(1-self.beta1)*gradients[\"db\" + str(l)]/(1-(self.beta1)**self.t)\n",
        "          parameters[\"b\" + str(l)] = (1-self.wd)*parameters[\"b\" + str(l)] - self.lr * (m_hat_fin) / (np.sqrt(v_hat) + self.eps)\n",
        "        \n",
        "        \n",
        "        return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Our Model on Fashion-MNIST**\n",
        "Trial cell to check the implementation of all the above functions"
      ],
      "metadata": {
        "id": "xYSsaAlPX2xq"
      },
      "id": "xYSsaAlPX2xq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8bfffb4",
      "metadata": {
        "id": "b8bfffb4"
      },
      "outputs": [],
      "source": [
        "# #Model Architechture\n",
        "\n",
        "# n = [784,[64,32],10]\n",
        "# activation_func = ['sigmoid','sigmoid','softmax']\n",
        "# loss = 'MSE'\n",
        "# batch_size = 200\n",
        "# learning_rate = 0.001\n",
        "# # optimiser = SGD(lr = learning_rate)\n",
        "# # optimiser = Momentum(lr = learning_rate, beta = 0.2)\n",
        "# # optimiser = Nesterov(lr = learning_rate, gamma = 0.9)\n",
        "# # optimiser = RMSprop(lr = learning_rate, decay_rate = 0.1,eps = 1e-6)\n",
        "# # optimiser = Adam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "# optimiser = Nadam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "\n",
        "\n",
        "# epochs = 30\n",
        "# m = X.shape[1]\n",
        "# parameters = initialize_parameters(n[0],n[1],n[2], 'Random')\n",
        "# # parameters = parameters_test2\n",
        "# count = 0\n",
        "\n",
        "# #loss array \n",
        "# # losses = np.array([])\n",
        "\n",
        "# while count < epochs :\n",
        "#   training_loss = 0\n",
        "#   count = count+1\n",
        "#   l = len(parameters)//2\n",
        "#   for i in np.arange(0, X.shape[1], batch_size):\n",
        "#     batch_count = batch_size\n",
        "#     if i + batch_size > X.shape[1]:\n",
        "#       batch_count = X.shape[1] - i + 1\n",
        "#     batch_size = batch_count\n",
        "\n",
        "#     layer_wise_outputs = ForwardPropagation(X[:,i:i+batch_size], parameters, activation_func)  \n",
        "#     gradients = BackPropagate(parameters, layer_wise_outputs, X[:,i:i+batch_size], y[:,i:i+batch_size], activation_func, loss)\n",
        "#     parameters = optimiser.update(parameters, gradients)\n",
        "#     training_loss = training_loss + cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(l)],loss)\n",
        "#   print(\"Loss after \"+ str(count) +\"th epoch =\" +str(training_loss*(batch_size)/m))\n",
        "\n",
        "\n",
        "\n",
        "# test_outputs = ForwardPropagation(X_test, parameters, activation_func)\n",
        "\n",
        "\n",
        "# def softmax_to_label(softmax_output):\n",
        "#   max_index = np.argmax(softmax_output, axis = 0)\n",
        "#   return max_index\n",
        "\n",
        "# test_outputs['a'+str(len(parameters)//2)] = softmax_to_label(test_outputs['a'+str(len(parameters)//2)])\n",
        "\n",
        "\n",
        "# def accuracy_score(y_true, y_pred):\n",
        "#     correct = np.sum(y_true == y_pred)\n",
        "#     total = len(y_true)\n",
        "#     accuracy = correct / total\n",
        "#     return accuracy\n",
        "\n",
        "# # print(softmax_to_label(y_test.T).shape)\n",
        "# # print(test_outputs['a'+str(len(n))].shape)\n",
        "# y_test_fin = softmax_to_label(y_test.T)\n",
        "# print(f\"Test Accuracy = {100*accuracy_score( y_test_fin, test_outputs['a'+str(len(parameters)//2)])} %\")\n",
        "\n",
        "# def accuracy_score(y_true, y_pred):\n",
        "#     pred_labels = np.argmax(y_pred, axis=0)\n",
        "#     return np.sum(pred_labels == y_true) / len(y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Neural Network Class**\n",
        "\n",
        "Implentation of the class using all the functions generated above. This class has 2 methods. \n",
        "####Class Attributes\n",
        "\n",
        "While initisaling the network, this is the convention I followed. Network Class takes a dictionary N as input which has information of the input size, size of hidden layers, output size, activation function at each layer, weight initialisation.\n",
        "\n",
        "\n",
        "```\n",
        "> N = {'n' : [784,[64,64,64],10],\n",
        "          'activation_func' : ['sigmoid','sigmoid','sigmoid','softmax'],\n",
        "          'initialisation' : 'Random'\n",
        "      }\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####Methods of the class\n",
        "\n",
        "\n",
        "1.  **Train:**\n",
        " \n",
        " Fits the model to the given dataset. Takes Train Dataset, Validation Data set, Batch size, Optimiser, Number of epochs as it's input arguments and fits model's parameters to the train data.\n",
        "\n",
        " \n",
        "2.   **Test:**\n",
        "\n",
        "        Tests the model on Test dataset given as input attributes to this method with the existing parameters.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v-Psftw7R2Kt"
      },
      "id": "v-Psftw7R2Kt"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Network():\n",
        "  def __init__(self, N, log):\n",
        "    self.n = N['n']\n",
        "    self.parameters = initialize_parameters(self.n[0],self.n[1],self.n[2], N['initialisation'])\n",
        "    self.activation_func = N['activation_func']\n",
        "    self.l = len(self.parameters)//2\n",
        "    self.log = log\n",
        "\n",
        "  def train(self,X,y,X_val,y_val,loss, batch_size, optimiser, epochs):\n",
        "    m = X.shape[1]\n",
        "    count = 0\n",
        "    while(count < epochs):\n",
        "      count = count+1\n",
        "      training_loss = 0\n",
        "      training_score = 0\n",
        "      for i in np.arange(start=0, stop=X.shape[1], step=batch_size):\n",
        "        batch_count = batch_size\n",
        "        if i + batch_size > X.shape[1]:\n",
        "          batch_count = X.shape[1] - i + 1\n",
        "        layer_wise_outputs = ForwardPropagation(X[:,i:i+batch_size], self.parameters, self.activation_func)  \n",
        "        gradients = BackPropagate(self.parameters, layer_wise_outputs, X[:,i:i+batch_size], y[:,i:i+batch_size], self.activation_func, loss)\n",
        "        self.parameters = optimiser.update(self.parameters, gradients)\n",
        "        training_loss = training_loss + cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(self.l)],loss)\n",
        "        training_score = training_score+ accuracy_score(softmax_to_label(y[:,i:i+batch_size]),  layer_wise_outputs['a'+str(self.l)])\n",
        "        \n",
        "      training_loss_fin = training_loss/np.ceil(m/batch_size)\n",
        "      print(\"Epoch:\"+str(count))\n",
        "      print(\"Training Loss after \"+ str(count) +\"th epoch =\" +str(training_loss_fin))\n",
        "\n",
        "      training_score_fin = training_score/np.ceil(m/batch_size)\n",
        "      print(\"Training score after \"+ str(count) +\"th epoch =\" + str(100*training_score_fin))\n",
        "\n",
        "      validation_outputs = ForwardPropagation(X_val, self.parameters, self.activation_func)\n",
        "      validation_loss = cost(y_val, validation_outputs['a'+str(self.l)],loss)\n",
        "      print(\"Validation Loss after \"+ str(count) +\"th epoch =\" + str(validation_loss))\n",
        "      validation_score = accuracy_score(softmax_to_label(y_val), validation_outputs['a'+str(self.l)])\n",
        "      print(\"Validation Score after \"+ str(count) +\"th epoch =\" + str(100*validation_score))\n",
        "      if(self.log == True):\n",
        "        metrics = {\"train loss\": training_loss_fin, \"train score\": training_score_fin , \"val loss\": validation_loss, \"accuracy\": validation_score}\n",
        "        wandb.log(metrics)\n",
        "\n",
        "\n",
        "    # def val_test(self,X,Y):\n",
        "    #   test_outputs = ForwardPropagation(X, self.parameters, self.activation_func)\n",
        "    #   print(f\"Val Accuracy = {100*accuracy_score(softmax_to_label(Y), test_outputs['a'+str(self.l)])} %\")\n",
        "    #   if(self.log == True):\n",
        "    #     wandb.log({'Val Accuracy' : accuracy_score(softmax_to_label(Y), test_outputs['a'+str(self.l)])})\n",
        "    \n",
        "  def test(self,X,Y):\n",
        "    test_outputs = ForwardPropagation(X, self.parameters, self.activation_func)\n",
        "    print(f\"Test Accuracy = {100*accuracy_score(Y, test_outputs['a'+str(self.l)])} %\")\n",
        "    if(self.log == True):\n",
        "      wandb.log({'Test Accuracy' : accuracy_score(Y, test_outputs['a'+str(self.l)])})   "
      ],
      "metadata": {
        "id": "pHlE5wt4fh54"
      },
      "id": "pHlE5wt4fh54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_val, y_train,y_val  = train_test_split(X.T, y.T, test_size=0.1)\n",
        "# X_train, X_val, y_train,y_val  = X_train.T, X_val.T, y_train.T,y_val.T \n",
        "\n",
        "# N = {'n' : [784,[64,64,64],10],\n",
        "#      'activation_func' : ['sigmoid','sigmoid','sigmoid','softmax'],\n",
        "#      'initialisation' : 'Random'\n",
        "#     }\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# epochs = 5\n",
        "# loss = 'CE'\n",
        "# learning_rate = 1e-3\n",
        "# # optimiser = SGD(lr = learning_rate, weight_decay = 0.0005)\n",
        "# # optimiser = Momentum(lr = learning_rate, beta = 0.9, weight_decay = 0)\n",
        "# # optimiser = Nesterov(lr = learning_rate, gamma = 0.9, weight_decay = 0)\n",
        "# optimiser = RMSprop(lr = learning_rate, decay_rate = 0.9,eps = 1e-6, weight_decay = 0)\n",
        "# # optimiser = Adam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6, weight_decay = 0)\n",
        "# # optimiser = Nadam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6, weight_decay = 0.0005)\n",
        "\n",
        "\n",
        "# network = Network(N,False)\n",
        "# network.train(X_train , y_train, X_val, y_val, loss, batch_size, optimiser, epochs)\n",
        "# network.test(X_test, y_test)"
      ],
      "metadata": {
        "id": "VT9vGGoqf84W"
      },
      "id": "VT9vGGoqf84W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Helper Functions -2** "
      ],
      "metadata": {
        "id": "-Ltbb0LbLXOI"
      },
      "id": "-Ltbb0LbLXOI"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_optimiser(optimiser_str,learning_rate,weight_decay):\n",
        "  if optimiser_str == 'sgd':\n",
        "    optimiser = SGD(lr = learning_rate, weight_decay=weight_decay)\n",
        "  elif optimiser_str == 'momentum':\n",
        "    optimiser = Momentum(lr = learning_rate, beta = 0.9, weight_decay=weight_decay)\n",
        "  elif optimiser_str == 'nesterov':\n",
        "    optimiser = Nesterov(lr = learning_rate, gamma = 0.9, weight_decay=weight_decay)\n",
        "  elif optimiser_str == 'rmsprop':\n",
        "    optimiser = RMSprop(lr = learning_rate, decay_rate = 0.1,eps = 1e-6, weight_decay=weight_decay)\n",
        "  elif optimiser_str == 'adam':\n",
        "    optimiser = Adam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6, weight_decay=weight_decay)\n",
        "  elif optimiser_str == 'nadam':\n",
        "    optimiser = Nadam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6, weight_decay=weight_decay)\n",
        "  return optimiser\n"
      ],
      "metadata": {
        "id": "aT2fOszBLZ82"
      },
      "id": "aT2fOszBLZ82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4 - Sweeping through Hyper-parameters** \n",
        "\n",
        "Implements various sweep methods in wandb.\n",
        "\n",
        "Report and Results can be found here: [CS6910 Assignment 1- Wandb Experiments and Report ](https://wandb.ai/berserank/CS6910%20Assignment%201)"
      ],
      "metadata": {
        "id": "2WqyREUKeR1T"
      },
      "id": "2WqyREUKeR1T"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_val, y_train,y_val  = train_test_split(X.T, y.T, test_size=0.1)\n",
        "X_train, X_val, y_train,y_val  = X_train.T, X_val.T, y_train.T,y_val.T \n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name' : 'Hyperparameter Tuning-Random Sweep-2'\n",
        "}\n",
        "\n",
        "# metric = {\n",
        "#     'name': 'accuracy',\n",
        "#     'goal': 'maximize'   \n",
        "#     }\n",
        "\n",
        "# sweep_config['metric'] = metric\n",
        "\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimiser': {\n",
        "        'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "    'layer_size': {\n",
        "        'values': [32,64,128]\n",
        "        },\n",
        "    'epochs': {\n",
        "          'values': [5,10]\n",
        "        },\n",
        "    'hidden_layers': {\n",
        "          'values': [3,4,5]\n",
        "        },\n",
        "    'learning_rate': {\n",
        "          'values': [1e-3,1e-4]\n",
        "        },\n",
        "    'weight_initialisation': {\n",
        "          'values': ['Xavier']\n",
        "        },\n",
        "    'activation_functions': {\n",
        "          'values': ['relu','leakyRelu']\n",
        "        },\n",
        "    'batch_size': {\n",
        "          'values': [16,32,64]\n",
        "        },\n",
        "    'weight_decay': {\n",
        "          'values': [0,0.0001]\n",
        "        }\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 Assignment 1\")\n",
        "\n",
        "\n",
        "def train_sweep(config=None):\n",
        "  with wandb.init(config=config) as run:\n",
        "    config = wandb.config\n",
        "    \n",
        "    N = {'n' : [784,config.hidden_layers*[config.layer_size],10],\n",
        "          'activation_func' : config.hidden_layers*[str(config.activation_functions)]+['softmax'],\n",
        "          'initialisation' : config.weight_initialisation\n",
        "        } \n",
        "\n",
        "    exp_name = 'e_'+ str(config.epochs)+'_hl_'+str(config.hidden_layers)\n",
        "    exp_name = exp_name + '_ls_'+ str(config.layer_size)\n",
        "    exp_name = exp_name+'_bs_'+ str(config.batch_size)+'_ac_'+str(config.activation_functions)+'_optim_'+ config.optimiser+'_in_'+config.weight_initialisation\n",
        "    \n",
        "    wandb.run.name = exp_name\n",
        "\n",
        "\n",
        "    batch_size = config.batch_size\n",
        "    epochs = config.epochs\n",
        "    loss = 'CE'\n",
        "    optimiser = build_optimiser(config.optimiser, config.learning_rate, config.weight_decay)\n",
        "\n",
        "    network = Network(N,True)\n",
        "    network.train(X_train , y_train, X_val, y_val, loss, batch_size, optimiser, epochs)\n",
        "    network.test(X_test, y_test)\n",
        "\n",
        "wandb.agent(sweep_id, train_sweep, count= 40)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "CkVW6P28eP9w"
      },
      "id": "CkVW6P28eP9w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 7 - Plotting the Confusion Matrix**\n",
        "\n",
        "We achieved best performance with the following metrics:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "*   Epochs = 10\n",
        "*   Hidden layers = [64,64,64]\n",
        "*   Batch Size = 32\n",
        "*   Activation Function for hidden layers = Leaky ReLU\n",
        "*   Optimiser = Nadam\n",
        "*   Learning rate = 1e-3\n",
        "*   Weight decay = 0\n",
        "*   Loss = Cross Entropy\n",
        "*   Weight Initialisation = Xavier\n",
        "*   Validation Accuracy = 88.88%\n",
        "```\n",
        "Plotting the confusion matrix after fitting our network to this model gave 87.1% accuracy on test data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M4erg2YXucZQ"
      },
      "id": "M4erg2YXucZQ"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train,y_val  = train_test_split(X.T, y.T, test_size=0.1)\n",
        "X_train, X_val, y_train,y_val  = X_train.T, X_val.T, y_train.T,y_val.T \n",
        "\n",
        "N = {'n' : [784,[64,64,64],10],\n",
        "     'activation_func' : ['leakyRelu','leakyRelu','leakyRelu','softmax'],\n",
        "     'initialisation' : 'Xavier'\n",
        "    }\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "loss = 'CE'\n",
        "learning_rate = 1e-3\n",
        "# optimiser = SGD(lr = learning_rate, weight_decay = 0.0005)\n",
        "# optimiser = Momentum(lr = learning_rate, beta = 0.9, weight_decay = 0)\n",
        "# optimiser = Nesterov(lr = learning_rate, gamma = 0.9, weight_decay = 0)\n",
        "# optimiser = RMSprop(lr = learning_rate, decay_rate = 0.9,eps = 1e-6, weight_decay = 0)\n",
        "# optimiser = Adam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6, weight_decay = 0)\n",
        "optimiser = Nadam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6, weight_decay = 0)\n",
        "\n",
        "\n",
        "network = Network(N,False)\n",
        "network.train(X_train , y_train, X_val, y_val, loss, batch_size, optimiser, epochs)\n",
        "network.test(X_test, y_test)"
      ],
      "metadata": {
        "id": "nXRjoIDOuZ_S"
      },
      "id": "nXRjoIDOuZ_S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_outputs = ForwardPropagation(X_test, network.parameters, network.activation_func)\n",
        "predicted_outputs = softmax_to_label(predicted_outputs['a'+str(len(network.parameters)//2)])\n",
        "# confusion_matrix = metrics.confusion_matrix(y_test, predicted_outputs)\n",
        "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = classes)\n",
        "# cm_display.plot()\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "fig = ConfusionMatrixDisplay.from_predictions(y_true = y_test, y_pred = predicted_outputs, display_labels = classes, cmap = 'PuBu')\n",
        "fig = fig.ax_.get_figure() \n",
        "fig.set_figwidth(10)\n",
        "fig.set_figheight(10)  \n",
        "wandb.log({'Confusion Matrix : Fashion-MNIST':fig})"
      ],
      "metadata": {
        "id": "PZmI7MEEzHW7"
      },
      "id": "PZmI7MEEzHW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 8- MSE Loss Models**\n",
        "\n",
        "In theory, CE loss is typically used for classification tasks where the output of the model is a probability distribution over a set of classes . CE loss punishes the difference more than MSE loss as the goal is to minimize the difference between the predicted probability distribution and the true probability distribution of the labels. In contrast, MSE loss is typically used for regression tasks where the goal is to minimize the difference between the predicted and true values.\n",
        "\n",
        "\n",
        "To compare a model using MSE as loss with a model using CE as loss, I am evaluating the performance of two models that have the same hyperparameters, except that one uses cross-entropy loss and the other uses mean squared error (MSE) loss. I am choosing the best models from both the bayesian sweeps for this purpose.  \n",
        "\n"
      ],
      "metadata": {
        "id": "PsC24kKpBqHl"
      },
      "id": "PsC24kKpBqHl"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_val, y_train,y_val  = train_test_split(X.T, y.T, test_size=0.1)\n",
        "X_train, X_val, y_train,y_val  = X_train.T, X_val.T, y_train.T,y_val.T \n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'Hyperparameter Tuning- Bayesian Sweep - MSE Loss- RL'\n",
        "}\n",
        "\n",
        "metric = {\n",
        "    'name': 'accuracy',\n",
        "    'goal': 'maximize'   \n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimiser': {\n",
        "        'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "    'layer_size': {\n",
        "        'values': [32,64,128]\n",
        "        },\n",
        "    'epochs': {\n",
        "          'values': [5,10]\n",
        "        },\n",
        "    'hidden_layers': {\n",
        "          'values': [3,4,5]\n",
        "        },\n",
        "    'learning_rate': {\n",
        "          'values': [1e-3,1e-4]\n",
        "        },\n",
        "    'weight_initialisation': {\n",
        "          'values': ['Random', 'Xavier']\n",
        "        },\n",
        "    'activation_functions': {\n",
        "          'values': ['relu','leakyRelu']\n",
        "        },\n",
        "    'batch_size': {\n",
        "          'values': [16,32,64]\n",
        "        },\n",
        "    'weight_decay': {\n",
        "          'values': [0,0.0005]\n",
        "        }\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 Assignment 1\")\n",
        "\n",
        "\n",
        "def train_sweep(config=None):\n",
        "  with wandb.init(config=config) as run:\n",
        "    config = wandb.config\n",
        "    \n",
        "    N = {'n' : [784,config.hidden_layers*[config.layer_size],10],\n",
        "          'activation_func' : config.hidden_layers*[str(config.activation_functions)]+['softmax'],\n",
        "          'initialisation' : config.weight_initialisation\n",
        "        } \n",
        "\n",
        "    exp_name = 'e_'+ str(config.epochs)+'_hl_'+str(config.hidden_layers)\n",
        "    exp_name = exp_name + '_ls_'+ str(config.layer_size)\n",
        "    exp_name = exp_name+'_bs_'+ str(config.batch_size)+'_ac_'+str(config.activation_functions)+'_optim_'+ config.optimiser+'_in_'+config.weight_initialisation\n",
        "    \n",
        "    wandb.run.name = exp_name\n",
        "\n",
        "\n",
        "    batch_size = config.batch_size\n",
        "    epochs = config.epochs\n",
        "    loss = 'MSE'\n",
        "    optimiser = build_optimiser(config.optimiser, config.learning_rate, config.weight_decay)\n",
        "\n",
        "    network = Network(N,True)\n",
        "    network.train(X_train , y_train, X_val, y_val, loss, batch_size, optimiser, epochs)\n",
        "    network.test(X_test, y_test)\n",
        "\n",
        "wandb.agent(sweep_id, train_sweep, count= 40)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "0ODRbe1vBgUT"
      },
      "id": "0ODRbe1vBgUT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best accuracy of 82.08% was achieved on validation set with MSE Loss for the following hyper-parameters after performing bayesian sweep over the given hyper parameters.\n",
        "\n",
        "```\n",
        "*   Epochs = 5\n",
        "*   Hidden layers = [128,128,128,128,128]\n",
        "*   Batch Size = 16\n",
        "*   Activation Function for hidden layers = Tanh\n",
        "*   Optimiser = Nadam\n",
        "*   Learning rate = 1e-4\n",
        "*   Weight decay = 0\n",
        "*   Loss = MSE\n",
        "*   Weight Initialisation = Xavier\n",
        "\n",
        "```\n",
        "\n",
        "Running a new experiment with the same parameters but with CE loss and comparing it with MSE loss. Results can be found in the [wandb report](https://wandb.ai/berserank/CS6910%20Assignment%201)"
      ],
      "metadata": {
        "id": "WzxzL9OIj0RW"
      },
      "id": "WzxzL9OIj0RW"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_val, y_train,y_val  = train_test_split(X.T, y.T, test_size=0.1)\n",
        "X_train, X_val, y_train,y_val  = X_train.T, X_val.T, y_train.T,y_val.T \n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'name' : 'MSE vs CE'\n",
        "}\n",
        "\n",
        "parameters_dict = {\n",
        "    'loss': {\n",
        "        'values': ['CE', 'MSE']\n",
        "        }\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 Assignment 1\")\n",
        "\n",
        "\n",
        "def train_sweep(config=None):\n",
        "  with wandb.init(config=config) as run:\n",
        "    config = wandb.config\n",
        "    \n",
        "    N = {'n' : [784,[128,128,128],10],\n",
        "          'activation_func' : 3*['sigmoid']+['softmax'],\n",
        "          'initialisation' : 'Xavier'\n",
        "        } \n",
        "\n",
        "    exp_name = 'e_5_hl_5_ls_128_bs_16_ac_tanh_optim_nadam_in_Xavier_loss_' + config.loss\n",
        "    wandb.run.name = exp_name\n",
        "\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 10\n",
        "    loss = config.loss\n",
        "    optimiser = Adam(lr = 1e-3, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6, weight_decay=0)\n",
        "\n",
        "    network = Network(N,True)\n",
        "    network.train(X_train , y_train, X_val, y_val, loss, batch_size, optimiser, epochs)\n",
        "    network.test(X_test, y_test)\n",
        "\n",
        "wandb.agent(sweep_id, train_sweep, count= 100)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Vl0SBA5Rjr-A"
      },
      "id": "Vl0SBA5Rjr-A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 10- MNIST Dataset**\n",
        "\n",
        "I am choosing the below three architechtures to test on MNIST dataset: These were the best performing models with a certain activation function. Results can be found in the [wandb report](https://wandb.ai/berserank/CS6910%20Assignment%201)\n",
        "\n",
        "|  | Architechture 1 | Architechture 2 | Architechture 3|\n",
        "|----------|----------|----------|----------|\n",
        "| Epochs | 10 | 10 | 10 |\n",
        "| Hidden Layers | [64,64,64] | [128,128,128] |  [128,128,128]|\n",
        "| Batch Size | 32 | 32 | 32|\n",
        "| Activation function | Leaky ReLU |  Leaky ReLU | ReLU |\n",
        "| Optimiser | Nadam | Adam | Adam |\n",
        "| Learning Rate | 1e-3 | 1e-3 | 1e-3 |\n",
        "| Weight Decay | 0 | 0.0001 | 0.0001 |\n",
        "| Loss | Cross Entropy | Cross Entropy | Cross Entropy |\n",
        "| Weight Initialisation | Xavier | Xavier | Xavier |\n",
        "| Accuracy on Fashion-MNIST | 88.88 | 88.73 | 88.15  |\n",
        "| Accuracy on MNIST Test data| 96.85 | 97.42| 97.07 |\n",
        "\n"
      ],
      "metadata": {
        "id": "RlPT201WB2ei"
      },
      "id": "RlPT201WB2ei"
    },
    {
      "cell_type": "code",
      "source": [
        "(X, y), (X_test, y_test) = mnist.load_data()\n",
        "X = X.reshape(X.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "def one_hot_encode(y):\n",
        "  encoded_array = np.zeros((y.size, y.max()+1), dtype=int)\n",
        "  encoded_array[np.arange(y.size),y] = 1 \n",
        "  return encoded_array\n",
        "\n",
        "def softmax_to_label(softmax_output):\n",
        "  max_index = np.argmax(softmax_output, axis = 0)\n",
        "  return max_index\n",
        "\n",
        "X = X.T/255\n",
        "X_test = X_test.T/255\n",
        "y = one_hot_encode(y)\n",
        "y = y.T"
      ],
      "metadata": {
        "id": "ickBAeNFBply"
      },
      "id": "ickBAeNFBply",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train,y_val  = train_test_split(X.T, y.T, test_size=0.1)\n",
        "X_train, X_val, y_train,y_val  = X_train.T, X_val.T, y_train.T,y_val.T \n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'name' : 'Performance check-MNIST Dataset'\n",
        "}\n",
        "\n",
        "N1 = {'N': [784,[64,64,64],10], 'activation_func': 'leakyRelu', 'optimiser': 'nadam'} \n",
        "N2 = {'N': [784,[128,128,128],10], 'activation_func': 'leakyRelu', 'optimiser': 'adam'} \n",
        "N3 = {'N': [784,[128,128,128],10], 'activation_func': 'relu', 'optimiser': 'adam'} \n",
        "\n",
        "\n",
        "parameters_dict = {\n",
        "    'architechture': {\n",
        "        'values': [N1, N2, N3]\n",
        "        }\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 Assignment 1\")\n",
        "\n",
        "def train_sweep(config=None):\n",
        "  with wandb.init(config=config) as run:\n",
        "    config = wandb.config\n",
        "    \n",
        "    N = {'n' : config.architechture['N'],\n",
        "          'activation_func' : [config.architechture['activation_func']]*3+['softmax'],\n",
        "          'initialisation' : 'Xavier'\n",
        "        } \n",
        "\n",
        "    exp_name = str(config.architechture)\n",
        "    wandb.run.name = exp_name\n",
        "\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 10\n",
        "    loss = 'CE'\n",
        "    if (config.architechture['optimiser'] == 'nadam'):\n",
        "      optimiser = build_optimiser(config.architechture['optimiser'], learning_rate = 1e-3, weight_decay = 0)\n",
        "    elif (config.architechture['optimiser'] == 'adam'):\n",
        "      optimiser = build_optimiser(config.architechture['optimiser'], learning_rate = 1e-3, weight_decay = 0.0001)\n",
        "\n",
        "    network = Network(N,True)\n",
        "    network.train(X_train , y_train, X_val, y_val, loss, batch_size, optimiser, epochs)\n",
        "    # network.val_test(X_val,y_val)\n",
        "    network.test(X_test, y_test)\n",
        "\n",
        "    #Confusion Matrix\n",
        "    predicted_outputs = ForwardPropagation(X_test, network.parameters, network.activation_func)\n",
        "    predicted_outputs = softmax_to_label(predicted_outputs['a'+str(len(network.parameters)//2)])\n",
        "    fig = ConfusionMatrixDisplay.from_predictions(y_true = y_test, y_pred = predicted_outputs, cmap = 'PuBu')\n",
        "    fig = fig.ax_.get_figure() \n",
        "    fig.set_figwidth(10)\n",
        "    fig.set_figheight(10)  \n",
        "    wandb.log({'Confusion Matrix':fig})\n",
        "\n",
        "wandb.agent(sweep_id, train_sweep)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "xUr-pNsYCBXM"
      },
      "id": "xUr-pNsYCBXM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8FSpqS1yK-ax",
        "00f21563",
        "4612eae7",
        "-Ltbb0LbLXOI"
      ]
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}