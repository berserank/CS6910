{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "36603b73",
      "metadata": {
        "id": "36603b73"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-Processing**"
      ],
      "metadata": {
        "id": "4r2LM0FSQZJq"
      },
      "id": "4r2LM0FSQZJq"
    },
    {
      "cell_type": "code",
      "source": [
        "(X, y), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X = X.reshape(X.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "#One Hot Encoding for y\n",
        "def one_hot_encode(y):\n",
        "  encoded_array = np.zeros((y.size, y.max()+1), dtype=int)\n",
        "  encoded_array[np.arange(y.size),y] = 1 \n",
        "  return encoded_array\n",
        "\n",
        "X = X/255\n",
        "X_test = X_test/255\n",
        "y = one_hot_encode(y)\n",
        "y_test = one_hot_encode(y_test)\n",
        "\n",
        "\n",
        "X = X.T\n",
        "X_test = X_test.T\n",
        "y = y.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCRZSoXLOgDK",
        "outputId": "b183f236-fd2a-4468-8e80-c29448c9fe2c"
      },
      "id": "yCRZSoXLOgDK",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 784)\n",
            "(60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b5e764",
      "metadata": {
        "id": "f1b5e764"
      },
      "source": [
        "**Activation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "181c79d9",
      "metadata": {
        "id": "181c79d9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    #print(-z)\n",
        "    return 1 / (1 + np.exp(-(z)))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z):\n",
        "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92521085",
      "metadata": {
        "id": "92521085"
      },
      "source": [
        "**Weight Initialisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7b6e46f4",
      "metadata": {
        "id": "7b6e46f4"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(input_size, n, output_size):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.random.randn(n[0],input_size)*0.01\n",
        "    parameters['b'+str(1)] = np.random.randn(n[0],1)\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.random.randn(n[i],n[i-1])*0.01\n",
        "        parameters['b'+str(i+1)] = np.random.randn(n[i],1)\n",
        "    parameters['W'+str(len(n)+1)] = np.random.randn(output_size,n[-1])*0.01\n",
        "    parameters['b'+str(len(n)+1)] = np.random.randn(output_size,1)\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "affa846c",
      "metadata": {
        "id": "affa846c"
      },
      "source": [
        "**Forward Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c2eb9262",
      "metadata": {
        "id": "c2eb9262"
      },
      "outputs": [],
      "source": [
        "def linear(W, X, b, activation_func):\n",
        "    #print(f\"W Shape = {W.shape}, X Shape= {X.shape}, W= {W}, X = {X}, b = {b} \" )\n",
        "    h = np.matmul(W,X)+b\n",
        "    if activation_func == 'sigmoid':\n",
        "        #print(h)\n",
        "        a = sigmoid(h)\n",
        "    elif activation_func == 'relu':\n",
        "        a = relu(h)\n",
        "    elif activation_func == 'tanh':\n",
        "        a = tanh(h)\n",
        "    elif activation_func == 'softmax':\n",
        "        a = softmax(h)\n",
        "    return h,a\n",
        "\n",
        "def ForwardPropagation(X, parameters, activation_func):\n",
        "    layer_wise_outputs = {}\n",
        "    layer_wise_outputs['h1'], layer_wise_outputs['a1'] = linear(parameters['W1'], X, parameters['b1'], activation_func[0])\n",
        "    for i in range(1, (len(parameters)//2)):\n",
        "        layer_wise_outputs['h'+str(i+1)], layer_wise_outputs['a'+str(i+1)] = linear(parameters['W'+str(i+1)],layer_wise_outputs['a'+str(i)],parameters['b'+str(i+1)], activation_func[i])\n",
        "    return layer_wise_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f21563",
      "metadata": {
        "id": "00f21563"
      },
      "source": [
        "**Loss Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1f926743",
      "metadata": {
        "id": "1f926743"
      },
      "outputs": [],
      "source": [
        "def MSELoss(Y, Y_pred):\n",
        "    MSE = np.mean((Y - Y_pred) ** 2)\n",
        "    return MSE\n",
        "\n",
        "def CrossEntropyLoss(Y, Y_pred):\n",
        "    CE = [-Y[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "    crossEntropy = np.mean(CE)\n",
        "    return crossEntropy\n",
        "\n",
        "def cost(Y, Y_pred, loss_func):\n",
        "    if (loss_func == 'MSE'):\n",
        "        return (MSELoss(Y, Y_pred))\n",
        "    elif (loss_func == 'CE'):\n",
        "        return (CrossEntropyLoss(Y, Y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4612eae7",
      "metadata": {
        "id": "4612eae7"
      },
      "source": [
        "**Back Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6b0f3183",
      "metadata": {
        "id": "6b0f3183"
      },
      "outputs": [],
      "source": [
        "def ActivationBackward(dA, Z, activation_func) :\n",
        "    \n",
        "    if (activation_func == 'sigmoid'):\n",
        "        grad = sigmoid(Z)*(1-sigmoid(Z))\n",
        "       \n",
        "    elif (activation_func == 'relu'):\n",
        "        grad = np.where(Z>0, 1, 0)\n",
        "        \n",
        "    elif (activation_func == 'tanh'):\n",
        "        grad = 1 - tanh(Z)**2\n",
        "    elif (activation_func == 'softmax'):\n",
        "        grad = softmax(Z) * (1-softmax(Z))\n",
        "    dZ = dA * grad\n",
        "    return dZ\n",
        "\n",
        "def softmax_derivative(x):\n",
        "    return softmax(x) * (1-softmax(x))        \n",
        "    \n",
        "def LayerBackward(dZl, Wl, bl, A_prev):\n",
        "    \n",
        "    m = A_prev.shape[1]\n",
        "    \n",
        "    dWl = (1/m) * np.matmul(dZl, A_prev.T)\n",
        "    dbl = (1/m)* np.sum(dZl, axis=1, keepdims=True)\n",
        "    dA_prev = np.matmul(Wl.T,dZl)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dWl.shape == Wl.shape)\n",
        "    assert (dbl.shape == bl.shape)\n",
        "    return dWl, dbl, dA_prev\n",
        "   \n",
        "def BackPropagate(parameters, layer_wise_outputs,X, Y, activation_func, loss):\n",
        "    gradients = {}\n",
        "    l = len(layer_wise_outputs)//2\n",
        "    m = Y.shape[1]\n",
        "    AL = layer_wise_outputs['a'+str(l)]\n",
        "    HL = layer_wise_outputs['h'+str(l)]\n",
        "    \n",
        "    if loss == 'CE':\n",
        "        gradients['dh'+str(l)] = AL-Y\n",
        "    elif loss == 'MSE':\n",
        "        gradients['dh'+str(l)] = (AL-Y) * softmax_derivative(HL)\n",
        "        \n",
        "    for i in range(l-1,0,-1):\n",
        "        gradients['dW'+str(i+1)],gradients['db'+str(i+1)],gradients['da'+str(i)] = LayerBackward(gradients['dh'+str(i+1)], parameters['W'+str(i+1)], parameters['b'+str(i+1)], layer_wise_outputs['a'+str(i)])\n",
        "        gradients['dh'+str(i)] = ActivationBackward(gradients['da'+str(i)], layer_wise_outputs['h'+ str(i)] , activation_func[i-1])\n",
        "        \n",
        "    gradients['dW'+str(1)],gradients['db'+str(1)],gradients['da'+str(0)] = LayerBackward(gradients['dh'+str(1)], parameters['W'+str(1)], parameters['b'+str(1)], X)    \n",
        "    \n",
        "    return gradients\n",
        "\n",
        "\n",
        "# parameters = initialize_parameters(2, [1,2,3], 2)\n",
        "# activation_func = ['sigmoid','sigmoid','sigmoid','softmax']\n",
        "# X = np.array([1,2]).reshape(2,1)  \n",
        "# Y = np.array([1,2]).reshape(2,1)\n",
        "# loss = 'CE'\n",
        "\n",
        "# layer_wise_outputs = ForwardPropagation(X, parameters, activation_func)\n",
        "# print(layer_wise_outputs)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss))\n",
        "# print(parameters['W3'].shape)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss)['dW3'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3648a8",
      "metadata": {
        "id": "1c3648a8"
      },
      "source": [
        "**Optimisers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "90e72095",
      "metadata": {
        "id": "90e72095"
      },
      "outputs": [],
      "source": [
        "def sgd(parameters,gradients,learning_rate):\n",
        "    L = len(parameters) // 2 \n",
        "    for l in range(1, L + 1):\n",
        "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * gradients[\"dW\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * gradients[\"db\" + str(l)]\n",
        "    return parameters\n",
        "\n",
        "def mgd(parameters, gradients, learning_rate, beta, previous_updates):\n",
        "     L = len(parameters) // 2 \n",
        "     for l in range(1, L + 1):\n",
        "        previous_updates[\"W\"+str(l)] = 0\n",
        "        previous_updates[\"b\"+str(l)] = 0\n",
        "     for l in range(1, L + 1):\n",
        "        previous_updates[\"W\"+str(l)] = beta*previous_updates[\"W\"+str(l)] + (1-beta)*gradients[\"dW\" + str(l)]\n",
        "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * previous_updates[\"W\"+str(l)]\n",
        "        previous_updates[\"b\"+str(l)] = beta*previous_updates[\"b\"+str(l)] + (1-beta)*gradients[\"db\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * previous_updates[\"b\"+str(l)]\n",
        "     return parameters\n",
        "\n",
        "def nesterov(parameters,gradients,learning_rate, beta, )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Our Model on Fashion-MNIST**\n"
      ],
      "metadata": {
        "id": "xYSsaAlPX2xq"
      },
      "id": "xYSsaAlPX2xq"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b8bfffb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8bfffb4",
        "outputId": "8caa3b7d-98fb-4345-b2a3-e0e8e2f6caff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 1th epoch =0.23085865607662218\n",
            "Loss after 2th epoch =0.22995540218201152\n",
            "Loss after 3th epoch =0.1952936231283036\n",
            "Loss after 4th epoch =0.15742045627571466\n",
            "Loss after 5th epoch =0.13673232254235865\n",
            "Loss after 6th epoch =0.10294145145691803\n",
            "Loss after 7th epoch =0.08296023919947375\n",
            "Loss after 8th epoch =0.07104423060796418\n",
            "Loss after 9th epoch =0.06268464180247475\n",
            "Loss after 10th epoch =0.056950837986175375\n",
            "Loss after 11th epoch =0.0528853940208869\n",
            "Loss after 12th epoch =0.04999485644650049\n",
            "Loss after 13th epoch =0.04780004552293244\n",
            "Loss after 14th epoch =0.046059707395922515\n",
            "Loss after 15th epoch =0.044613069856188556\n",
            "Loss after 16th epoch =0.043358746874043314\n",
            "Loss after 17th epoch =0.04224194305819924\n",
            "Loss after 18th epoch =0.04123319586554396\n",
            "Loss after 19th epoch =0.040316774595407764\n",
            "Loss after 20th epoch =0.03948356009736844\n",
            "Loss after 21th epoch =0.03872726228681425\n",
            "Loss after 22th epoch =0.0380416318681412\n",
            "Loss after 23th epoch =0.03741696394361086\n",
            "Loss after 24th epoch =0.036843485545350885\n",
            "Loss after 25th epoch =0.03631324376354692\n",
            "Loss after 26th epoch =0.035819948035048656\n",
            "Loss after 27th epoch =0.0353586546196019\n",
            "Loss after 28th epoch =0.0349254323112066\n",
            "Loss after 29th epoch =0.034517064852581866\n",
            "Loss after 30th epoch =0.034130833143483516\n"
          ]
        }
      ],
      "source": [
        "#Model Architechture\n",
        "n = [784,[64,32],10]\n",
        "activation_func = ['sigmoid','sigmoid','softmax']\n",
        "loss = 'CE'\n",
        "batch_size = 200\n",
        "learning_rate = 0.25\n",
        "epochs = 30\n",
        "\n",
        "#\n",
        "m = X.shape[1]\n",
        "parameters = initialize_parameters(n[0],n[1],n[2])\n",
        "count = 0\n",
        "\n",
        "#loss array \n",
        "# losses = np.array([])\n",
        "\n",
        "while count < epochs :\n",
        "  training_loss = 0\n",
        "  count = count+1\n",
        "  previous_updates = {}\n",
        "  for i in np.arange(0, X.shape[1], batch_size):\n",
        "    batch_count = batch_size\n",
        "    if i + batch_size > X.shape[1]:\n",
        "      batch_count = X.shape[1] - i + 1\n",
        "    batch_size = batch_count\n",
        "    layer_wise_outputs = ForwardPropagation(X[:,i:i+batch_size], parameters, activation_func)  \n",
        "    gradients = BackPropagate(parameters, layer_wise_outputs, X[:,i:i+batch_size], y[:,i:i+batch_size], activation_func, loss)\n",
        "    # parameters = mgd(parameters,gradients,learning_rate,0.2,previous_updates)\n",
        "    parameters = sgd(parameters,gradients,learning_rate)\n",
        "    losses = np.append(losses, cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(len(n))],loss))\n",
        "    training_loss = training_loss + cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(len(n))],loss)\n",
        "  print(\"Loss after \"+ str(count) +\"th epoch =\" +str(training_loss*(batch_size)/m))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_outputs = ForwardPropagation(X_test, parameters, activation_func)\n",
        "\n",
        "def softmax_to_label(softmax_output):\n",
        "    max_index = np.argmax(softmax_output, axis = 0)\n",
        "    return max_index\n",
        "\n",
        "test_outputs['a'+str(len(n))] = softmax_to_label(test_outputs['a'+str(len(n))])\n",
        "\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    total = len(y_true)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "print(softmax_to_label(y_test.T).shape)\n",
        "print(test_outputs['a'+str(len(n))].shape)\n",
        "y_test_fin = softmax_to_label(y_test.T)\n",
        "print(f\"Test Accuracy = {100*accuracy_score( y_test_fin, test_outputs['a'+str(len(n))])} %\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W2jKXIMuF57",
        "outputId": "4b6d4252-66db-43a5-dac3-3c3c51c005bd"
      },
      "id": "9W2jKXIMuF57",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n",
            "(10000,)\n",
            "Test Accuracy = 86.27 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}