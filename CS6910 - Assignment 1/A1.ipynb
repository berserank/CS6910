{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "36603b73",
      "metadata": {
        "id": "36603b73"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-Processing**"
      ],
      "metadata": {
        "id": "4r2LM0FSQZJq"
      },
      "id": "4r2LM0FSQZJq"
    },
    {
      "cell_type": "code",
      "source": [
        "(X, y), (X_test, y_test) = fashion_mnist.load_data()\n",
        "sample = 9\n",
        "image = X[sample]\n",
        "fig = plt.figure\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yCRZSoXLOgDK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "00a085ea-8cdf-4954-db78-9f3954059601"
      },
      "id": "yCRZSoXLOgDK",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.reshape(X.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "#One Hot Encoding for y\n",
        "def one_hot_encode(y):\n",
        "  encoded_array = np.zeros((y.size, y.max()+1), dtype=int)\n",
        "  encoded_array[np.arange(y.size),y] = 1 \n",
        "  return encoded_array\n",
        "\n",
        "def softmax_to_label(softmax_output):\n",
        "  max_index = np.argmax(softmax_output, axis = 0)\n",
        "  return max_index\n",
        "\n",
        "\n",
        "X = X/255\n",
        "X_test = X_test/255\n",
        "y = one_hot_encode(y)\n",
        "y_test = one_hot_encode(y_test)\n",
        "\n",
        "\n",
        "X = X.T\n",
        "X_test = X_test.T\n",
        "y = y.T"
      ],
      "metadata": {
        "id": "WaHeSKrhH0nK"
      },
      "id": "WaHeSKrhH0nK",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f1b5e764",
      "metadata": {
        "id": "f1b5e764"
      },
      "source": [
        "**Activation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "181c79d9",
      "metadata": {
        "id": "181c79d9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    #print(-z)\n",
        "    return 1 / (1 + np.exp(-(z)))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z):\n",
        "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
        "\n",
        "def softmax(x):\n",
        "    # x = np.float128(x)\n",
        "    temp = np.exp(x-np.max(x, axis = 0))\n",
        "    fin = temp/temp.sum(axis = 0)\n",
        "    \n",
        "    return fin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92521085",
      "metadata": {
        "id": "92521085"
      },
      "source": [
        "**Weight Initialisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "7b6e46f4",
      "metadata": {
        "id": "7b6e46f4"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(input_size, n, output_size, initialisation):\n",
        "  if (initialisation == 'Random'):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.random.randn(n[0],input_size)*0.01\n",
        "    parameters['b'+str(1)] = np.random.randn(n[0],1)\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.random.randn(n[i],n[i-1])*0.01\n",
        "        parameters['b'+str(i+1)] = np.random.randn(n[i],1)\n",
        "    parameters['W'+str(len(n)+1)] = np.random.randn(output_size,n[-1])*0.01\n",
        "    parameters['b'+str(len(n)+1)] = np.random.randn(output_size,1)\n",
        "\n",
        "  elif (initialisation == 'Xavier'):\n",
        "    parameters = {}\n",
        "    m = np.sqrt(6)/(input_size+n[0])\n",
        "    parameters['W'+str(1)] = np.random.uniform(-m,m, (n[0],input_size))\n",
        "    parameters['b'+str(1)] = np.random.randn(n[0],1)\n",
        "    for i in range(1,len(n)):\n",
        "        m = np.sqrt(6)/(n[i-1]+n[i])\n",
        "        parameters['W'+str(i+1)] = np.random.uniform(-m,m, (n[i],n[i-1]) )\n",
        "        parameters['b'+str(i+1)] = np.random.randn(n[i],1)\n",
        "    m = np.sqrt(6)/(output_size+n[-1])\n",
        "    parameters['W'+str(len(n)+1)] = np.random.uniform(-m,m,(output_size,n[-1]))\n",
        "    parameters['b'+str(len(n)+1)] = np.random.randn(output_size,1)\n",
        "\n",
        "  return parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def initialize_parameters_zeros(input_size, n, output_size):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.zeros((n[0],input_size))\n",
        "    parameters['b'+str(1)] = np.zeros((n[0],1))\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.zeros((n[i],n[i-1]))\n",
        "        parameters['b'+str(i+1)] = np.zeros((n[i],1))\n",
        "    parameters['W'+str(len(n)+1)] = np.zeros((output_size,n[-1]))\n",
        "    parameters['b'+str(len(n)+1)] = np.zeros((output_size,1))\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "affa846c",
      "metadata": {
        "id": "affa846c"
      },
      "source": [
        "**Forward Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c2eb9262",
      "metadata": {
        "id": "c2eb9262"
      },
      "outputs": [],
      "source": [
        "def linear(W, X, b, activation_func):\n",
        "    #print(f\"W Shape = {W.shape}, X Shape= {X.shape}, W= {W}, X = {X}, b = {b} \" )\n",
        "    h = np.matmul(W,X)+b\n",
        "    if activation_func == 'sigmoid':\n",
        "        #print(h)\n",
        "        a = sigmoid(h)\n",
        "    elif activation_func == 'relu':\n",
        "        a = relu(h)\n",
        "    elif activation_func == 'tanh':\n",
        "        a = tanh(h)\n",
        "    elif activation_func == 'softmax':\n",
        "        a = softmax(h)\n",
        "    return h,a\n",
        "\n",
        "def ForwardPropagation(X, parameters, activation_func):\n",
        "    layer_wise_outputs = {}\n",
        "    layer_wise_outputs['h1'], layer_wise_outputs['a1'] = linear(parameters['W1'], X, parameters['b1'], activation_func[0])\n",
        "    for i in range(1, (len(parameters)//2)):\n",
        "        layer_wise_outputs['h'+str(i+1)], layer_wise_outputs['a'+str(i+1)] = linear(parameters['W'+str(i+1)],layer_wise_outputs['a'+str(i)],parameters['b'+str(i+1)], activation_func[i])\n",
        "    return layer_wise_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f21563",
      "metadata": {
        "id": "00f21563"
      },
      "source": [
        "**Loss Functions and Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "1f926743",
      "metadata": {
        "id": "1f926743"
      },
      "outputs": [],
      "source": [
        "def MSELoss(Y, Y_pred):\n",
        "    MSE = np.mean((Y - Y_pred) ** 2, axis = 1)\n",
        "    MSE = np.mean(MSE)\n",
        "    return MSE\n",
        "\n",
        "def CrossEntropyLoss(Y, Y_pred):\n",
        "    CE = [-Y[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "    crossEntropy = np.mean(CE)\n",
        "    return crossEntropy\n",
        "\n",
        "def cost(Y, Y_pred, loss_func):\n",
        "    if (loss_func == 'MSE'):\n",
        "        return (MSELoss(Y, Y_pred))\n",
        "    elif (loss_func == 'CE'):\n",
        "        return (CrossEntropyLoss(Y, Y_pred))\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "  correct = np.sum(y_true == y_pred)\n",
        "  total = len(y_true)\n",
        "  accuracy = correct / total\n",
        "  return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4612eae7",
      "metadata": {
        "id": "4612eae7"
      },
      "source": [
        "**Back Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6b0f3183",
      "metadata": {
        "id": "6b0f3183"
      },
      "outputs": [],
      "source": [
        "def ActivationBackward(dA, Z, activation_func) :\n",
        "    \n",
        "    if (activation_func == 'sigmoid'):\n",
        "        grad = sigmoid(Z)*(1-sigmoid(Z))\n",
        "       \n",
        "    elif (activation_func == 'relu'):\n",
        "        grad = np.where(Z>0, 1, 0)\n",
        "        \n",
        "    elif (activation_func == 'tanh'):\n",
        "        grad = 1 - tanh(Z)**2\n",
        "    elif (activation_func == 'softmax'):\n",
        "        grad = softmax(Z) * (1-softmax(Z))\n",
        "    dZ = dA * grad\n",
        "    return dZ\n",
        "\n",
        "def softmax_derivative(x):\n",
        "    return softmax(x) * (1-softmax(x))        \n",
        "    \n",
        "def LayerBackward(dZl, Wl, bl, A_prev):\n",
        "    \n",
        "    m = A_prev.shape[1]\n",
        "    # print(m)\n",
        "    dWl = (1/m) * np.matmul(dZl, A_prev.T)\n",
        "    dbl = (1/m)* np.sum(dZl, axis=1, keepdims=True)\n",
        "    dA_prev = np.matmul(Wl.T,dZl)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dWl.shape == Wl.shape)\n",
        "    assert (dbl.shape == bl.shape)\n",
        "    return dWl, dbl, dA_prev\n",
        "   \n",
        "def BackPropagate(parameters, layer_wise_outputs,X, Y, activation_func, loss):\n",
        "    gradients = {}\n",
        "    l = len(layer_wise_outputs)//2\n",
        "    m = Y.shape[1]\n",
        "    AL = layer_wise_outputs['a'+str(l)]\n",
        "    HL = layer_wise_outputs['h'+str(l)]\n",
        "    \n",
        "    if loss == 'CE':\n",
        "        gradients['dh'+str(l)] = AL-Y\n",
        "    elif loss == 'MSE':\n",
        "        gradients['dh'+str(l)] = (AL-Y) * softmax_derivative(HL)\n",
        "        \n",
        "    for i in range(l-1,0,-1):\n",
        "        gradients['dW'+str(i+1)],gradients['db'+str(i+1)],gradients['da'+str(i)] = LayerBackward(gradients['dh'+str(i+1)], parameters['W'+str(i+1)], parameters['b'+str(i+1)], layer_wise_outputs['a'+str(i)])\n",
        "        gradients['dh'+str(i)] = ActivationBackward(gradients['da'+str(i)], layer_wise_outputs['h'+ str(i)] , activation_func[i-1])\n",
        "        \n",
        "    gradients['dW'+str(1)],gradients['db'+str(1)],gradients['da'+str(0)] = LayerBackward(gradients['dh'+str(1)], parameters['W'+str(1)], parameters['b'+str(1)], X)    \n",
        "    \n",
        "    return gradients\n",
        "\n",
        "\n",
        "# parameters = initialize_parameters(2, [1,2,3], 2)\n",
        "# activation_func = ['sigmoid','sigmoid','sigmoid','softmax']\n",
        "# X = np.array([1,2]).reshape(2,1)  \n",
        "# Y = np.array([1,2]).reshape(2,1)\n",
        "# loss = 'CE'\n",
        "\n",
        "# layer_wise_outputs = ForwardPropagation(X, parameters, activation_func)\n",
        "# print(layer_wise_outputs)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss))\n",
        "# print(parameters['W3'].shape)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss)['dW3'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3648a8",
      "metadata": {
        "id": "1c3648a8"
      },
      "source": [
        "**Optimisers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "90e72095",
      "metadata": {
        "id": "90e72095"
      },
      "outputs": [],
      "source": [
        "class Optimiser:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SGD(Optimiser):\n",
        "    def __init__(self, lr):\n",
        "        super().__init__(lr)\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters) // 2 \n",
        "        for l in range(1, L + 1):\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * gradients[\"db\" + str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Momentum(Optimiser):\n",
        "    def __init__(self, lr, beta):\n",
        "        super().__init__(lr)\n",
        "        self.beta = beta\n",
        "        self.v = {}\n",
        "  \n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        for l in range(1, L + 1):\n",
        "          self.v[\"W\"+str(l)] = self.beta * self.v[\"W\"+str(l)] + (1) * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * self.v[\"W\"+str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.beta * self.v[\"b\"+str(l)] + (1) * gradients[\"db\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * self.v[\"b\"+str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Nesterov(Optimiser):\n",
        "    def __init__(self, lr, gamma):\n",
        "        super().__init__(lr)\n",
        "        self.gamma = gamma\n",
        "        self.look_ahead = {}\n",
        "        self.v = {}\n",
        "        \n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        if self.look_ahead == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.look_ahead[\"W\"+str(l)] = 0\n",
        "            self.look_ahead[\"b\"+str(l)] = 0\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          self.look_ahead[\"W\"+str(l)] = parameters[\"W\" + str(l)]-self.gamma*self.v[\"W\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = self.look_ahead[\"W\"+str(l)] - self.lr * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\"+str(l)] = self.gamma * self.v[\"W\"+str(l)] + self.lr * gradients[\"dW\" + str(l)]\n",
        "\n",
        "          self.look_ahead[\"b\"+str(l)] = parameters[\"b\" + str(l)]-self.gamma*self.v[\"b\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = self.look_ahead[\"b\"+str(l)] - self.lr * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.gamma * self.v[\"b\"+str(l)] + self.lr * gradients[\"db\" + str(l)]\n",
        "\n",
        "          \n",
        "        return parameters\n",
        "\n",
        "class RMSprop(Optimiser):\n",
        "    def __init__(self, lr, decay_rate, eps):\n",
        "        super().__init__(lr)\n",
        "        self.decay_rate = decay_rate\n",
        "        self.eps = eps\n",
        "        self.s = {}\n",
        "\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.s == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.s[\"W\"+str(l)] = 0\n",
        "            self.s[\"b\"+str(l)] = 0\n",
        "        for l in range(1, L + 1):\n",
        "          self.s[\"W\"+str(l)] = self.decay_rate * self.s[\"W\"+str(l)] + (1 - self.decay_rate) * (gradients[\"dW\" + str(l)]**2)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * gradients[\"dW\" + str(l)] / (np.sqrt(self.s[\"W\"+str(l)]) + self.eps)\n",
        "\n",
        "          self.s[\"b\"+str(l)] = self.decay_rate * self.s[\"b\"+str(l)] + (1 - self.decay_rate) * (gradients[\"db\" + str(l)]**2)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * gradients[\"db\" + str(l)] / (np.sqrt(self.s[\"b\"+str(l)]) + self.eps)\n",
        "        \n",
        "        return parameters\n",
        "        \n",
        "\n",
        "class Adam(Optimiser):\n",
        "    def __init__(self, lr, beta1, beta2, eps):\n",
        "        super().__init__(lr)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.m == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.m[\"W\"+str(l)] = 0\n",
        "            self.m[\"b\"+str(l)] = 0\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        self.t += 1\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          self.m[\"W\" + str(l)] = self.beta1 * self.m[\"W\" + str(l)] + (1 - self.beta1) * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\" + str(l)] = self.beta2 * self.v[\"W\" + str(l)] + (1 - self.beta2) * (gradients[\"dW\" + str(l)]**2)\n",
        "          m_hat = self.m[\"W\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"W\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "          self.m[\"b\" + str(l)] = self.beta1 * self.m[\"b\" + str(l)] + (1 - self.beta1) * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\" + str(l)] = self.beta2 * self.v[\"b\" + str(l)] + (1 - self.beta2) * (gradients[\"db\" + str(l)]**2)\n",
        "          m_hat = self.m[\"b\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"b\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "        \n",
        "        \n",
        "        return parameters\n",
        "\n",
        " \n",
        "class Nadam(Optimiser):\n",
        "    def __init__(self, lr, beta1, beta2, eps):\n",
        "        super().__init__(lr)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.m == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.m[\"W\"+str(l)] = 0\n",
        "            self.m[\"b\"+str(l)] = 0\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        self.t += 1\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          self.m[\"W\" + str(l)] = self.beta1 * self.m[\"W\" + str(l)] + (1 - self.beta1) * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\" + str(l)] = self.beta2 * self.v[\"W\" + str(l)] + (1 - self.beta2) * (gradients[\"dW\" + str(l)]**2)\n",
        "          m_hat = self.m[\"W\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"W\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          m_hat_fin = (self.beta1*m_hat)+(1-self.beta1)*gradients[\"dW\" + str(l)]/(1-(self.beta1)**self.t)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * (m_hat_fin) / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "          self.m[\"b\" + str(l)] = self.beta1 * self.m[\"b\" + str(l)] + (1 - self.beta1) * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\" + str(l)] = self.beta2 * self.v[\"b\" + str(l)] + (1 - self.beta2) * (gradients[\"db\" + str(l)]**2)\n",
        "          m_hat = self.m[\"b\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"b\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          m_hat_fin = (self.beta1*m_hat)+(1-self.beta1)*gradients[\"db\" + str(l)]/(1-(self.beta1)**self.t)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * (m_hat_fin) / (np.sqrt(v_hat) + self.eps)\n",
        "        \n",
        "        \n",
        "        return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Our Model on Fashion-MNIST**\n"
      ],
      "metadata": {
        "id": "xYSsaAlPX2xq"
      },
      "id": "xYSsaAlPX2xq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8bfffb4",
      "metadata": {
        "id": "b8bfffb4"
      },
      "outputs": [],
      "source": [
        "#Model Architechture\n",
        "\n",
        "n = [784,[64,32],10]\n",
        "activation_func = ['sigmoid','sigmoid','softmax']\n",
        "loss = 'MSE'\n",
        "batch_size = 200\n",
        "learning_rate = 0.001\n",
        "# optimiser = SGD(lr = learning_rate)\n",
        "# optimiser = Momentum(lr = learning_rate, beta = 0.2)\n",
        "# optimiser = Nesterov(lr = learning_rate, gamma = 0.9)\n",
        "# optimiser = RMSprop(lr = learning_rate, decay_rate = 0.1,eps = 1e-6)\n",
        "# optimiser = Adam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "optimiser = Nadam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "m = X.shape[1]\n",
        "parameters = initialize_parameters(n[0],n[1],n[2], 'Random')\n",
        "# parameters = parameters_test2\n",
        "count = 0\n",
        "\n",
        "#loss array \n",
        "# losses = np.array([])\n",
        "\n",
        "while count < epochs :\n",
        "  training_loss = 0\n",
        "  count = count+1\n",
        "  l = len(parameters)//2\n",
        "  for i in np.arange(0, X.shape[1], batch_size):\n",
        "    batch_count = batch_size\n",
        "    if i + batch_size > X.shape[1]:\n",
        "      batch_count = X.shape[1] - i + 1\n",
        "    batch_size = batch_count\n",
        "\n",
        "    layer_wise_outputs = ForwardPropagation(X[:,i:i+batch_size], parameters, activation_func)  \n",
        "    gradients = BackPropagate(parameters, layer_wise_outputs, X[:,i:i+batch_size], y[:,i:i+batch_size], activation_func, loss)\n",
        "    parameters = optimiser.update(parameters, gradients)\n",
        "    training_loss = training_loss + cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(l)],loss)\n",
        "  print(\"Loss after \"+ str(count) +\"th epoch =\" +str(training_loss*(batch_size)/m))\n",
        "\n",
        "\n",
        "\n",
        "test_outputs = ForwardPropagation(X_test, parameters, activation_func)\n",
        "\n",
        "\n",
        "def softmax_to_label(softmax_output):\n",
        "  max_index = np.argmax(softmax_output, axis = 0)\n",
        "  return max_index\n",
        "\n",
        "test_outputs['a'+str(len(parameters)//2)] = softmax_to_label(test_outputs['a'+str(len(parameters)//2)])\n",
        "\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    total = len(y_true)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# print(softmax_to_label(y_test.T).shape)\n",
        "# print(test_outputs['a'+str(len(n))].shape)\n",
        "y_test_fin = softmax_to_label(y_test.T)\n",
        "print(f\"Test Accuracy = {100*accuracy_score( y_test_fin, test_outputs['a'+str(len(parameters)//2)])} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Class**"
      ],
      "metadata": {
        "id": "v-Psftw7R2Kt"
      },
      "id": "v-Psftw7R2Kt"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Network():\n",
        "  def __init__(self, N):\n",
        "    self.n = N['n']\n",
        "    self.parameters = initialize_parameters_zeros(self.n[0],self.n[1],self.n[2])\n",
        "    self.activation_func = N['activation_func']\n",
        "    self.l = len(self.parameters)//2\n",
        "\n",
        "  def train(self,X,Y, initialisation, loss, batch_size, optimiser, epochs):\n",
        "    self.parameters = initialize_parameters(self.n[0],self.n[1],self.n[2], initialisation )\n",
        "    m = X.shape[1]\n",
        "    count = 0\n",
        "    while(count < epochs):\n",
        "      count = count+1\n",
        "      training_loss = 0\n",
        "      for i in np.arange(0, X.shape[1], batch_size):\n",
        "        batch_count = batch_size\n",
        "\n",
        "        if i + batch_size > X.shape[1]:\n",
        "          batch_count = X.shape[1] - i + 1\n",
        "        batch_size = batch_count\n",
        "        # print(self.parameters['W3'])\n",
        "        layer_wise_outputs = ForwardPropagation(X[:,i:i+batch_size], self.parameters, self.activation_func)  \n",
        "        \n",
        "        gradients = BackPropagate(self.parameters, layer_wise_outputs, X[:,i:i+batch_size], y[:,i:i+batch_size], self.activation_func, loss)\n",
        "        self.parameters = optimiser.update(self.parameters, gradients)\n",
        "        training_loss = training_loss + cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(self.l)],loss)\n",
        "      print(\"Loss after \"+ str(count) +\"th epoch =\" +str(training_loss*(batch_size)/m))\n",
        "\n",
        "  def test(self,X,Y):\n",
        "    test_outputs = ForwardPropagation(X, self.parameters, self.activation_func)\n",
        "    test_outputs['a'+str(self.l)] = softmax_to_label(test_outputs['a'+str(self.l)])\n",
        "    y_test_fin = softmax_to_label(y_test.T)\n",
        "    print(f\"Test Accuracy = {100*accuracy_score( y_test_fin, test_outputs['a'+str(self.l)])} %\")   "
      ],
      "metadata": {
        "id": "pHlE5wt4fh54"
      },
      "id": "pHlE5wt4fh54",
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = {'n' : [784,[64,32],10],\n",
        "     'activation_func' : ['sigmoid','sigmoid','softmax']\n",
        "     }\n",
        "\n",
        "\n",
        "initialisation = 'Random'\n",
        "batch_size = 200\n",
        "epochs = 30\n",
        "loss = 'CE'\n",
        "learning_rate = 0.001\n",
        "optimiser = Nadam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "\n",
        "\n",
        "network = Network(N)\n",
        "network.train(X, y, initialisation, loss, batch_size, optimiser, epochs)\n",
        "network.test(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT9vGGoqf84W",
        "outputId": "faa677a9-34e6-4587-955d-127e068663f0"
      },
      "id": "VT9vGGoqf84W",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 1th epoch =0.19972025896065695\n",
            "Loss after 2th epoch =0.11504871057513767\n",
            "Loss after 3th epoch =0.07792617872401338\n",
            "Loss after 4th epoch =0.06264720425940802\n",
            "Loss after 5th epoch =0.05189517538709645\n",
            "Loss after 6th epoch =0.046502955106637\n",
            "Loss after 7th epoch =0.04294790227374406\n",
            "Loss after 8th epoch =0.040403458037017156\n",
            "Loss after 9th epoch =0.03839718539135832\n",
            "Loss after 10th epoch =0.03675541141443851\n",
            "Loss after 11th epoch =0.03539357257741813\n",
            "Loss after 12th epoch =0.034236630362564587\n",
            "Loss after 13th epoch =0.03323036881715163\n",
            "Loss after 14th epoch =0.03234228126738461\n",
            "Loss after 15th epoch =0.03154813663464999\n",
            "Loss after 16th epoch =0.030829113465535466\n",
            "Loss after 17th epoch =0.030171659107045147\n",
            "Loss after 18th epoch =0.02956566631374361\n",
            "Loss after 19th epoch =0.02900214202413552\n",
            "Loss after 20th epoch =0.028473297953605422\n",
            "Loss after 21th epoch =0.027973582284306447\n",
            "Loss after 22th epoch =0.02749987230745739\n",
            "Loss after 23th epoch =0.027050105557498504\n",
            "Loss after 24th epoch =0.02662120924305579\n",
            "Loss after 25th epoch =0.026210581242768507\n",
            "Loss after 26th epoch =0.02581643384058846\n",
            "Loss after 27th epoch =0.025437313518543794\n",
            "Loss after 28th epoch =0.02507200535931363\n",
            "Loss after 29th epoch =0.024719394007361034\n",
            "Loss after 30th epoch =0.024378457008376365\n",
            "Test Accuracy = 87.53999999999999 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}