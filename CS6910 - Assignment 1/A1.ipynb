{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T7vQy_csoJzQ"
      },
      "id": "T7vQy_csoJzQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36603b73",
      "metadata": {
        "id": "36603b73"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "LBKx5QKLECrv"
      },
      "id": "LBKx5QKLECrv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(project=\"CS6910 Assignment 1\")"
      ],
      "metadata": {
        "id": "v_fnfKBJEIhw"
      },
      "id": "v_fnfKBJEIhw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Data**"
      ],
      "metadata": {
        "id": "4r2LM0FSQZJq"
      },
      "id": "4r2LM0FSQZJq"
    },
    {
      "cell_type": "code",
      "source": [
        "(X, y), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "yCRZSoXLOgDK"
      },
      "id": "yCRZSoXLOgDK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 - Plotting MNIST Fashion Dataset**"
      ],
      "metadata": {
        "id": "lJs0eq3Xpat7"
      },
      "id": "lJs0eq3Xpat7"
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "fig, ax = plt.subplots(2,5,figsize=(20, 10))\n",
        "ax = ax.reshape(-1)\n",
        "\n",
        "for i in range(10):\n",
        "  sample = np.array(np.where(y == i)).reshape(-1)\n",
        "  sample = sample[3]\n",
        "  image = X[sample]\n",
        "  ax[i].set_title(classes[i])\n",
        "  ax[i].imshow(image)\n",
        "  \n",
        "plt.show()\n",
        "wandb.log({'Labels':fig})"
      ],
      "metadata": {
        "id": "Xvk0PjtzpiZE"
      },
      "id": "Xvk0PjtzpiZE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-Processing**"
      ],
      "metadata": {
        "id": "QsXHPBlepobh"
      },
      "id": "QsXHPBlepobh"
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.reshape(X.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "#One Hot Encoding for y\n",
        "def one_hot_encode(y):\n",
        "  encoded_array = np.zeros((y.size, y.max()+1), dtype=int)\n",
        "  encoded_array[np.arange(y.size),y] = 1 \n",
        "  return encoded_array\n",
        "\n",
        "def softmax_to_label(softmax_output):\n",
        "  max_index = np.argmax(softmax_output, axis = 0)\n",
        "  return max_index\n",
        "\n",
        "\n",
        "X = X/255\n",
        "X_test = X_test/255\n",
        "y = one_hot_encode(y)\n",
        "\n",
        "\n",
        "X = X.T\n",
        "X_test = X_test.T\n",
        "y = y.T"
      ],
      "metadata": {
        "id": "WaHeSKrhH0nK"
      },
      "id": "WaHeSKrhH0nK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f1b5e764",
      "metadata": {
        "id": "f1b5e764"
      },
      "source": [
        "**Activation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "181c79d9",
      "metadata": {
        "id": "181c79d9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    #print(-z)\n",
        "    return 1 / (1 + np.exp(-(z)))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z):\n",
        "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
        "\n",
        "def softmax(x):\n",
        "    # x = np.float128(x)\n",
        "    temp = np.exp(x-np.max(x, axis = 0))\n",
        "    fin = temp/temp.sum(axis = 0)\n",
        "    \n",
        "    return fin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92521085",
      "metadata": {
        "id": "92521085"
      },
      "source": [
        "**Weight Initialisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6e46f4",
      "metadata": {
        "id": "7b6e46f4"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(input_size, n, output_size, initialisation):\n",
        "  if (initialisation == 'Random'):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.random.randn(n[0],input_size)*0.01\n",
        "    parameters['b'+str(1)] = np.random.randn(n[0],1)\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.random.randn(n[i],n[i-1])*0.01\n",
        "        parameters['b'+str(i+1)] = np.random.randn(n[i],1)\n",
        "    parameters['W'+str(len(n)+1)] = np.random.randn(output_size,n[-1])*0.01\n",
        "    parameters['b'+str(len(n)+1)] = np.random.randn(output_size,1)\n",
        "\n",
        "  elif (initialisation == 'Xavier'):\n",
        "    parameters = {}\n",
        "    m = np.sqrt(6)/(input_size+n[0])\n",
        "    parameters['W'+str(1)] = np.random.uniform(-m,m, (n[0],input_size))\n",
        "    parameters['b'+str(1)] = np.random.randn(n[0],1)\n",
        "    for i in range(1,len(n)):\n",
        "        m = np.sqrt(6)/(n[i-1]+n[i])\n",
        "        parameters['W'+str(i+1)] = np.random.uniform(-m,m, (n[i],n[i-1]) )\n",
        "        parameters['b'+str(i+1)] = np.random.randn(n[i],1)\n",
        "    m = np.sqrt(6)/(output_size+n[-1])\n",
        "    parameters['W'+str(len(n)+1)] = np.random.uniform(-m,m,(output_size,n[-1]))\n",
        "    parameters['b'+str(len(n)+1)] = np.random.randn(output_size,1)\n",
        "\n",
        "  return parameters\n",
        "\n",
        "def initialize_parameters_zeros(input_size, n, output_size):\n",
        "    parameters = {}\n",
        "    parameters['W'+str(1)] = np.zeros((n[0],input_size))\n",
        "    parameters['b'+str(1)] = np.zeros((n[0],1))\n",
        "    for i in range(1,len(n)):\n",
        "        parameters['W'+str(i+1)] = np.zeros((n[i],n[i-1]))\n",
        "        parameters['b'+str(i+1)] = np.zeros((n[i],1))\n",
        "    parameters['W'+str(len(n)+1)] = np.zeros((output_size,n[-1]))\n",
        "    parameters['b'+str(len(n)+1)] = np.zeros((output_size,1))\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "affa846c",
      "metadata": {
        "id": "affa846c"
      },
      "source": [
        "**Forward Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2eb9262",
      "metadata": {
        "id": "c2eb9262"
      },
      "outputs": [],
      "source": [
        "def linear(W, X, b, activation_func):\n",
        "    #print(f\"W Shape = {W.shape}, X Shape= {X.shape}, W= {W}, X = {X}, b = {b} \" )\n",
        "    h = np.matmul(W,X)+b\n",
        "    if activation_func == 'sigmoid':\n",
        "        #print(h)\n",
        "        a = sigmoid(h)\n",
        "    elif activation_func == 'relu':\n",
        "        a = relu(h)\n",
        "    elif activation_func == 'tanh':\n",
        "        a = tanh(h)\n",
        "    elif activation_func == 'softmax':\n",
        "        a = softmax(h)\n",
        "    return h,a\n",
        "\n",
        "def ForwardPropagation(X, parameters, activation_func):\n",
        "    layer_wise_outputs = {}\n",
        "    layer_wise_outputs['h1'], layer_wise_outputs['a1'] = linear(parameters['W1'], X, parameters['b1'], activation_func[0])\n",
        "    for i in range(1, (len(parameters)//2)):\n",
        "        layer_wise_outputs['h'+str(i+1)], layer_wise_outputs['a'+str(i+1)] = linear(parameters['W'+str(i+1)],layer_wise_outputs['a'+str(i)],parameters['b'+str(i+1)], activation_func[i])\n",
        "    return layer_wise_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f21563",
      "metadata": {
        "id": "00f21563"
      },
      "source": [
        "**Loss Functions and Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f926743",
      "metadata": {
        "id": "1f926743"
      },
      "outputs": [],
      "source": [
        "def MSELoss(Y, Y_pred):\n",
        "    MSE = np.mean((Y - Y_pred) ** 2, axis = 1)\n",
        "    MSE = np.mean(MSE)\n",
        "    return MSE\n",
        "\n",
        "def CrossEntropyLoss(Y, Y_pred):\n",
        "    CE = [-Y[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "    crossEntropy = np.mean(CE)\n",
        "    return crossEntropy\n",
        "\n",
        "def cost(Y, Y_pred, loss_func):\n",
        "    if (loss_func == 'MSE'):\n",
        "        return (MSELoss(Y, Y_pred))\n",
        "    elif (loss_func == 'CE'):\n",
        "        return (CrossEntropyLoss(Y, Y_pred))\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "  correct = np.sum(y_true == y_pred)\n",
        "  total = len(y_true)\n",
        "  accuracy = correct / total\n",
        "  return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4612eae7",
      "metadata": {
        "id": "4612eae7"
      },
      "source": [
        "**Back Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0f3183",
      "metadata": {
        "id": "6b0f3183"
      },
      "outputs": [],
      "source": [
        "def ActivationBackward(dA, Z, activation_func) :\n",
        "    \n",
        "    if (activation_func == 'sigmoid'):\n",
        "        grad = sigmoid(Z)*(1-sigmoid(Z))\n",
        "       \n",
        "    elif (activation_func == 'relu'):\n",
        "        grad = np.where(Z>0, 1, 0)\n",
        "        \n",
        "    elif (activation_func == 'tanh'):\n",
        "        grad = 1 - tanh(Z)**2\n",
        "    elif (activation_func == 'softmax'):\n",
        "        grad = softmax(Z) * (1-softmax(Z))\n",
        "    dZ = dA * grad\n",
        "    return dZ\n",
        "\n",
        "def softmax_derivative(x):\n",
        "    return softmax(x) * (1-softmax(x))        \n",
        "    \n",
        "def LayerBackward(dZl, Wl, bl, A_prev):\n",
        "    \n",
        "    m = A_prev.shape[1]\n",
        "    # print(m)\n",
        "    dWl = (1/m) * np.matmul(dZl, A_prev.T)\n",
        "    dbl = (1/m)* np.sum(dZl, axis=1, keepdims=True)\n",
        "    dA_prev = np.matmul(Wl.T,dZl)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dWl.shape == Wl.shape)\n",
        "    assert (dbl.shape == bl.shape)\n",
        "    return dWl, dbl, dA_prev\n",
        "   \n",
        "def BackPropagate(parameters, layer_wise_outputs,X, Y, activation_func, loss):\n",
        "    gradients = {}\n",
        "    l = len(layer_wise_outputs)//2\n",
        "    m = Y.shape[1]\n",
        "    AL = layer_wise_outputs['a'+str(l)]\n",
        "    HL = layer_wise_outputs['h'+str(l)]\n",
        "    \n",
        "    if loss == 'CE':\n",
        "        gradients['dh'+str(l)] = AL-Y\n",
        "    elif loss == 'MSE':\n",
        "        gradients['dh'+str(l)] = (AL-Y) * softmax_derivative(HL)\n",
        "        \n",
        "    for i in range(l-1,0,-1):\n",
        "        gradients['dW'+str(i+1)],gradients['db'+str(i+1)],gradients['da'+str(i)] = LayerBackward(gradients['dh'+str(i+1)], parameters['W'+str(i+1)], parameters['b'+str(i+1)], layer_wise_outputs['a'+str(i)])\n",
        "        gradients['dh'+str(i)] = ActivationBackward(gradients['da'+str(i)], layer_wise_outputs['h'+ str(i)] , activation_func[i-1])\n",
        "        \n",
        "    gradients['dW'+str(1)],gradients['db'+str(1)],gradients['da'+str(0)] = LayerBackward(gradients['dh'+str(1)], parameters['W'+str(1)], parameters['b'+str(1)], X)    \n",
        "    \n",
        "    return gradients\n",
        "\n",
        "\n",
        "# parameters = initialize_parameters(2, [1,2,3], 2)\n",
        "# activation_func = ['sigmoid','sigmoid','sigmoid','softmax']\n",
        "# X = np.array([1,2]).reshape(2,1)  \n",
        "# Y = np.array([1,2]).reshape(2,1)\n",
        "# loss = 'CE'\n",
        "\n",
        "# layer_wise_outputs = ForwardPropagation(X, parameters, activation_func)\n",
        "# print(layer_wise_outputs)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss))\n",
        "# print(parameters['W3'].shape)\n",
        "# print(BackPropagate(parameters, layer_wise_outputs, X, Y, activation_func, loss)['dW3'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3648a8",
      "metadata": {
        "id": "1c3648a8"
      },
      "source": [
        "**Optimisers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e72095",
      "metadata": {
        "id": "90e72095"
      },
      "outputs": [],
      "source": [
        "class Optimiser:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SGD(Optimiser):\n",
        "    def __init__(self, lr):\n",
        "        super().__init__(lr)\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters) // 2 \n",
        "        for l in range(1, L + 1):\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * gradients[\"db\" + str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Momentum(Optimiser):\n",
        "    def __init__(self, lr, beta):\n",
        "        super().__init__(lr)\n",
        "        self.beta = beta\n",
        "        self.v = {}\n",
        "  \n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        for l in range(1, L + 1):\n",
        "          self.v[\"W\"+str(l)] = self.beta * self.v[\"W\"+str(l)] + (1) * gradients[\"dW\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * self.v[\"W\"+str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.beta * self.v[\"b\"+str(l)] + (1) * gradients[\"db\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * self.v[\"b\"+str(l)]\n",
        "        return parameters\n",
        "\n",
        "class Nesterov(Optimiser):\n",
        "    def __init__(self, lr, gamma):\n",
        "        super().__init__(lr)\n",
        "        self.gamma = gamma\n",
        "        self.look_ahead = {}\n",
        "        self.v = {}\n",
        "        \n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        if self.look_ahead == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.look_ahead[\"W\"+str(l)] = 0\n",
        "            self.look_ahead[\"b\"+str(l)] = 0\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          self.look_ahead[\"W\"+str(l)] = parameters[\"W\" + str(l)]-self.gamma*self.v[\"W\" + str(l)]\n",
        "          parameters[\"W\" + str(l)] = self.look_ahead[\"W\"+str(l)] - self.lr * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\"+str(l)] = self.gamma * self.v[\"W\"+str(l)] + self.lr * gradients[\"dW\" + str(l)]\n",
        "\n",
        "          self.look_ahead[\"b\"+str(l)] = parameters[\"b\" + str(l)]-self.gamma*self.v[\"b\" + str(l)]\n",
        "          parameters[\"b\" + str(l)] = self.look_ahead[\"b\"+str(l)] - self.lr * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\"+str(l)] = self.gamma * self.v[\"b\"+str(l)] + self.lr * gradients[\"db\" + str(l)]\n",
        "\n",
        "          \n",
        "        return parameters\n",
        "\n",
        "class RMSprop(Optimiser):\n",
        "    def __init__(self, lr, decay_rate, eps):\n",
        "        super().__init__(lr)\n",
        "        self.decay_rate = decay_rate\n",
        "        self.eps = eps\n",
        "        self.s = {}\n",
        "\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.s == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.s[\"W\"+str(l)] = 0\n",
        "            self.s[\"b\"+str(l)] = 0\n",
        "        for l in range(1, L + 1):\n",
        "          self.s[\"W\"+str(l)] = self.decay_rate * self.s[\"W\"+str(l)] + (1 - self.decay_rate) * (gradients[\"dW\" + str(l)]**2)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * gradients[\"dW\" + str(l)] / (np.sqrt(self.s[\"W\"+str(l)]) + self.eps)\n",
        "\n",
        "          self.s[\"b\"+str(l)] = self.decay_rate * self.s[\"b\"+str(l)] + (1 - self.decay_rate) * (gradients[\"db\" + str(l)]**2)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * gradients[\"db\" + str(l)] / (np.sqrt(self.s[\"b\"+str(l)]) + self.eps)\n",
        "        \n",
        "        return parameters\n",
        "        \n",
        "\n",
        "class Adam(Optimiser):\n",
        "    def __init__(self, lr, beta1, beta2, eps):\n",
        "        super().__init__(lr)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.m == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.m[\"W\"+str(l)] = 0\n",
        "            self.m[\"b\"+str(l)] = 0\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        self.t += 1\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          self.m[\"W\" + str(l)] = self.beta1 * self.m[\"W\" + str(l)] + (1 - self.beta1) * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\" + str(l)] = self.beta2 * self.v[\"W\" + str(l)] + (1 - self.beta2) * (gradients[\"dW\" + str(l)]**2)\n",
        "          m_hat = self.m[\"W\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"W\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "          self.m[\"b\" + str(l)] = self.beta1 * self.m[\"b\" + str(l)] + (1 - self.beta1) * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\" + str(l)] = self.beta2 * self.v[\"b\" + str(l)] + (1 - self.beta2) * (gradients[\"db\" + str(l)]**2)\n",
        "          m_hat = self.m[\"b\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"b\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "        \n",
        "        \n",
        "        return parameters\n",
        "\n",
        " \n",
        "class Nadam(Optimiser):\n",
        "    def __init__(self, lr, beta1, beta2, eps):\n",
        "        super().__init__(lr)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        L = len(parameters)//2\n",
        "        if self.m == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.m[\"W\"+str(l)] = 0\n",
        "            self.m[\"b\"+str(l)] = 0\n",
        "        if self.v == {}:\n",
        "          for l in range(1, L + 1):\n",
        "            self.v[\"W\"+str(l)] = 0\n",
        "            self.v[\"b\"+str(l)] = 0\n",
        "        self.t += 1\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "          self.m[\"W\" + str(l)] = self.beta1 * self.m[\"W\" + str(l)] + (1 - self.beta1) * gradients[\"dW\" + str(l)]\n",
        "          self.v[\"W\" + str(l)] = self.beta2 * self.v[\"W\" + str(l)] + (1 - self.beta2) * (gradients[\"dW\" + str(l)]**2)\n",
        "          m_hat = self.m[\"W\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"W\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          m_hat_fin = (self.beta1*m_hat)+(1-self.beta1)*gradients[\"dW\" + str(l)]/(1-(self.beta1)**self.t)\n",
        "          parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - self.lr * (m_hat_fin) / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "          self.m[\"b\" + str(l)] = self.beta1 * self.m[\"b\" + str(l)] + (1 - self.beta1) * gradients[\"db\" + str(l)]\n",
        "          self.v[\"b\" + str(l)] = self.beta2 * self.v[\"b\" + str(l)] + (1 - self.beta2) * (gradients[\"db\" + str(l)]**2)\n",
        "          m_hat = self.m[\"b\" + str(l)] / (1 - self.beta1 ** self.t)\n",
        "          v_hat = self.v[\"b\" + str(l)] / (1 - self.beta2 ** self.t)\n",
        "          m_hat_fin = (self.beta1*m_hat)+(1-self.beta1)*gradients[\"db\" + str(l)]/(1-(self.beta1)**self.t)\n",
        "          parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - self.lr * (m_hat_fin) / (np.sqrt(v_hat) + self.eps)\n",
        "        \n",
        "        \n",
        "        return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Our Model on Fashion-MNIST**\n"
      ],
      "metadata": {
        "id": "xYSsaAlPX2xq"
      },
      "id": "xYSsaAlPX2xq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8bfffb4",
      "metadata": {
        "id": "b8bfffb4"
      },
      "outputs": [],
      "source": [
        "# #Model Architechture\n",
        "\n",
        "# n = [784,[64,32],10]\n",
        "# activation_func = ['sigmoid','sigmoid','softmax']\n",
        "# loss = 'MSE'\n",
        "# batch_size = 200\n",
        "# learning_rate = 0.001\n",
        "# # optimiser = SGD(lr = learning_rate)\n",
        "# # optimiser = Momentum(lr = learning_rate, beta = 0.2)\n",
        "# # optimiser = Nesterov(lr = learning_rate, gamma = 0.9)\n",
        "# # optimiser = RMSprop(lr = learning_rate, decay_rate = 0.1,eps = 1e-6)\n",
        "# # optimiser = Adam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "# optimiser = Nadam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "\n",
        "\n",
        "# epochs = 30\n",
        "# m = X.shape[1]\n",
        "# parameters = initialize_parameters(n[0],n[1],n[2], 'Random')\n",
        "# # parameters = parameters_test2\n",
        "# count = 0\n",
        "\n",
        "# #loss array \n",
        "# # losses = np.array([])\n",
        "\n",
        "# while count < epochs :\n",
        "#   training_loss = 0\n",
        "#   count = count+1\n",
        "#   l = len(parameters)//2\n",
        "#   for i in np.arange(0, X.shape[1], batch_size):\n",
        "#     batch_count = batch_size\n",
        "#     if i + batch_size > X.shape[1]:\n",
        "#       batch_count = X.shape[1] - i + 1\n",
        "#     batch_size = batch_count\n",
        "\n",
        "#     layer_wise_outputs = ForwardPropagation(X[:,i:i+batch_size], parameters, activation_func)  \n",
        "#     gradients = BackPropagate(parameters, layer_wise_outputs, X[:,i:i+batch_size], y[:,i:i+batch_size], activation_func, loss)\n",
        "#     parameters = optimiser.update(parameters, gradients)\n",
        "#     training_loss = training_loss + cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(l)],loss)\n",
        "#   print(\"Loss after \"+ str(count) +\"th epoch =\" +str(training_loss*(batch_size)/m))\n",
        "\n",
        "\n",
        "\n",
        "# test_outputs = ForwardPropagation(X_test, parameters, activation_func)\n",
        "\n",
        "\n",
        "def softmax_to_label(softmax_output):\n",
        "  max_index = np.argmax(softmax_output, axis = 0)\n",
        "  return max_index\n",
        "\n",
        "# test_outputs['a'+str(len(parameters)//2)] = softmax_to_label(test_outputs['a'+str(len(parameters)//2)])\n",
        "\n",
        "\n",
        "# def accuracy_score(y_true, y_pred):\n",
        "#     correct = np.sum(y_true == y_pred)\n",
        "#     total = len(y_true)\n",
        "#     accuracy = correct / total\n",
        "#     return accuracy\n",
        "\n",
        "# # print(softmax_to_label(y_test.T).shape)\n",
        "# # print(test_outputs['a'+str(len(n))].shape)\n",
        "# y_test_fin = softmax_to_label(y_test.T)\n",
        "# print(f\"Test Accuracy = {100*accuracy_score( y_test_fin, test_outputs['a'+str(len(parameters)//2)])} %\")\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    pred_labels = np.argmax(y_pred, axis=0)\n",
        "    return np.sum(pred_labels == y_true) / len(y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Class**"
      ],
      "metadata": {
        "id": "v-Psftw7R2Kt"
      },
      "id": "v-Psftw7R2Kt"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Network():\n",
        "  def __init__(self, N):\n",
        "    self.n = N['n']\n",
        "    self.parameters = initialize_parameters(self.n[0],self.n[1],self.n[2], N['initialisation'])\n",
        "    self.activation_func = N['activation_func']\n",
        "    self.l = len(self.parameters)//2\n",
        "\n",
        "  def train(self,X,y,X_val,y_val,loss, batch_size, optimiser, epochs):\n",
        "    m = X.shape[1]\n",
        "    count = 0\n",
        "    while(count < epochs):\n",
        "      count = count+1\n",
        "      training_loss = 0\n",
        "      for i in np.arange(0, X.shape[1], batch_size):\n",
        "        batch_count = batch_size\n",
        "        if i + batch_size > X.shape[1]:\n",
        "          batch_count = X.shape[1] - i + 1\n",
        "        batch_size = batch_count\n",
        "        layer_wise_outputs = ForwardPropagation(X[:,i:i+batch_size], self.parameters, self.activation_func)  \n",
        "        gradients = BackPropagate(self.parameters, layer_wise_outputs, X[:,i:i+batch_size], y[:,i:i+batch_size], self.activation_func, loss)\n",
        "        self.parameters = optimiser.update(self.parameters, gradients)\n",
        "        training_loss = training_loss + cost(y[:,i:i+batch_size], layer_wise_outputs['a'+str(self.l)],loss)\n",
        "\n",
        "      training_loss_fin = training_loss*(batch_size)/m\n",
        "      print(\"Epoch:\"+str(count))\n",
        "      print(\"Training Loss after \"+ str(count) +\"th epoch =\" +str(training_loss_fin))\n",
        "\n",
        "      training_outputs = ForwardPropagation(X, self.parameters, self.activation_func)\n",
        "      training_score = accuracy_score(softmax_to_label(y), training_outputs['a'+str(self.l)])\n",
        "      print(\"Training score after \"+ str(count) +\"th epoch =\" + str(100*training_score))\n",
        "\n",
        "      validation_outputs = ForwardPropagation(X_val, self.parameters, self.activation_func)\n",
        "      validation_loss = cost(y_val, validation_outputs['a'+str(self.l)],loss)\n",
        "      print(\"Validation Loss after \"+ str(count) +\"th epoch =\" + str(validation_loss))\n",
        "      validation_score = accuracy_score(softmax_to_label(y_val), validation_outputs['a'+str(self.l)])\n",
        "      print(\"Validation Score after \"+ str(count) +\"th epoch =\" + str(100*validation_score))\n",
        "\n",
        "      metrics = {\"train loss\": training_loss_fin, \"train score\": training_score , \"val loss\": validation_loss, \"val score\": validation_score}\n",
        "      wandb.log(metrics)\n",
        "\n",
        "\n",
        "  def val_test(self,X,Y):\n",
        "    test_outputs = ForwardPropagation(X, self.parameters, self.activation_func)\n",
        "    print(f\"Val Accuracy = {100*accuracy_score(softmax_to_label(Y), test_outputs['a'+str(self.l)])} %\")\n",
        "    wandb.log({'Val Accuracy' : accuracy_score(softmax_to_label(Y), test_outputs['a'+str(self.l)])})\n",
        "    \n",
        "  def test(self,X,Y):\n",
        "    test_outputs = ForwardPropagation(X, self.parameters, self.activation_func)\n",
        "    print(f\"Test Accuracy = {100*accuracy_score(Y, test_outputs['a'+str(self.l)])} %\")\n",
        "    wandb.log({'Test Accuracy' : accuracy_score(Y, test_outputs['a'+str(self.l)])})   "
      ],
      "metadata": {
        "id": "pHlE5wt4fh54"
      },
      "id": "pHlE5wt4fh54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train,y_val  = train_test_split(X.T, y.T, test_size=0.1)\n",
        "X_train, X_val, y_train,y_val  = X_train.T, X_val.T, y_train.T,y_val.T \n",
        "\n",
        "N = {'n' : [784,[128,128,64],10],\n",
        "     'activation_func' : ['relu','relu','relu','softmax'],\n",
        "     'initialisation' : 'Random'\n",
        "    }\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "loss = 'CE'\n",
        "learning_rate = 0.1\n",
        "optimiser = SGD(lr = learning_rate)\n",
        "\n",
        "\n",
        "network = Network(N)\n",
        "network.train(X_train , y_train, X_val, y_val, loss, batch_size, optimiser, epochs)\n",
        "network.test(X_test, y_test)"
      ],
      "metadata": {
        "id": "VT9vGGoqf84W"
      },
      "id": "VT9vGGoqf84W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4 - Sweeping through Hyper-parameters** "
      ],
      "metadata": {
        "id": "2WqyREUKeR1T"
      },
      "id": "2WqyREUKeR1T"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_val, y_train,y_val  = train_test_split(X.T, y.T, test_size=0.1)\n",
        "X_train, X_val, y_train,y_val  = X_train.T, X_val.T, y_train.T,y_val.T \n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name' : 'Hyperparameter Tuning'\n",
        "}\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimiser': {\n",
        "        'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "    'layer_size': {\n",
        "        'values': [32,64,128]\n",
        "        },\n",
        "    'epochs': {\n",
        "          'values': [5,10]\n",
        "        },\n",
        "    'hidden_layers': {\n",
        "          'values': [3,4,5]\n",
        "        },\n",
        "    'learning_rate': {\n",
        "          'values': [1e-3,1e-4]\n",
        "        },\n",
        "    'weight_initialisation': {\n",
        "          'values': ['Random', 'Xavier']\n",
        "        },\n",
        "    'activation_functions': {\n",
        "          'values': ['sigmoid', 'tanh', 'relu']\n",
        "        },\n",
        "    'batch_size': {\n",
        "          'values': [16,32,64]\n",
        "        },\n",
        "    'weight_decay': {\n",
        "          'values': [0,0.0005,0.5]\n",
        "        }\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 Assignment 1\")\n",
        "\n",
        "\n",
        "def build_optimiser(optimiser_str,learning_rate):\n",
        "  if optimiser_str == 'sgd':\n",
        "    optimiser = SGD(lr = learning_rate)\n",
        "  elif optimiser_str == 'momentum':\n",
        "    optimiser = Momentum(lr = learning_rate, beta = 0.2)\n",
        "  elif optimiser_str == 'nesterov':\n",
        "    optimiser = Nesterov(lr = learning_rate, gamma = 0.9)\n",
        "  elif optimiser_str == 'rmsprop':\n",
        "    optimiser = RMSprop(lr = learning_rate, decay_rate = 0.1,eps = 1e-6)\n",
        "  elif optimiser_str == 'adam':\n",
        "    optimiser = Adam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "  elif optimiser_str == 'nadam':\n",
        "    optimiser = Nadam(lr = learning_rate, beta1 = 0.9, beta2 = 0.99 ,eps = 1e-6)\n",
        "  return optimiser\n",
        "\n",
        "\n",
        "def train_sweep(config=None):\n",
        "  with wandb.init(config=config) as run:\n",
        "    config = wandb.config\n",
        "\n",
        "    # loader = build_dataset(config.batch_size)\n",
        "    # network = build_network(config.fc_layer_size, config.dropout)\n",
        "    # optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
        "    N = {'n' : [784,config.hidden_layers*[config.layer_size],10],\n",
        "          'activation_func' : config.hidden_layers*[str(config.activation_functions)]+['softmax'],\n",
        "          'initialisation' : config.weight_initialisation\n",
        "        } \n",
        "\n",
        "    exp_name = 'in_'+config.weight_initialisation+'_e_'+ str(config.epochs)+'_hl_'+str(config.hidden_layers)\n",
        "    exp_name = exp_name + '_ls_'+ str(config.layer_size)\n",
        "    exp_name = exp_name+'_bs_'+ str(config.batch_size)+'_ac_'+str(config.activation_functions)\n",
        "    \n",
        "    wandb.run.name = exp_name\n",
        "\n",
        "\n",
        "    batch_size = config.batch_size\n",
        "    epochs = config.epochs\n",
        "    loss = 'CE'\n",
        "    optimiser = build_optimiser(config.optimiser, config.learning_rate)\n",
        "\n",
        "    network = Network(N)\n",
        "    network.train(X_train , y_train, X_val, y_val, loss, batch_size, optimiser, epochs)\n",
        "    network.val_test(X_val,y_val)\n",
        "    network.test(X_test, y_test)\n",
        "\n",
        "wandb.agent(sweep_id, train_sweep, count=5)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "CkVW6P28eP9w"
      },
      "id": "CkVW6P28eP9w",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}